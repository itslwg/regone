% Created 2020-02-23 Sun 18:31
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\usepackage[margin=1.25in]{geometry} \usepackage{booktabs} \usepackage{graphicx} \usepackage{adjustbox} \usepackage{amsmath} \hypersetup{colorlinks=true,linkcolor=blue} \usepackage{amsthm} \newtheorem{definition}{Definition} \usepackage{bookmark}
\author{Ludde}
\date{\today}
\title{}
\hypersetup{
 pdfauthor={Ludde},
 pdftitle={},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.2 (Org mode 9.1.14)}, 
 pdflang={English}}
\begin{document}

\begin{titlepage}
\centering
\includegraphics[width=0.15\textwidth]{example-image-1x1}\par\vspace{1cm}
{\scshape\LARGE Kungliga Tekniska Högskolan \par}
\vspace{1cm}
{\scshape\Large SF2930 Regression Analysis \par}
\vspace{1.5cm}
{\huge\bfseries Report I \\  \par}
\vspace{2cm}
{\Large\itshape Isac Karlsson\\ Ludvig Wärnberg Gerdin}
\vfill
Examiner \par
\textsc{Tatjana Pavlenko}

\vfill

{\large \today\par}
\end{titlepage}

\newpage
\tableofcontents
\newpage

\section{Introduction and Project Goals}
\label{sec:orgb36b855}
\subsection{Introduction}
\label{sec:org85ae39b}
Our choice of scenario is Scenario I: Body fat assessment, which involves Large-Sample regression (p < n). 
According to the World Health organization (WHO) obesity, the state where excess body fat is causing
extensive health effects, is a large risk factor for some chronic diseases. Some examples are cancer
and diabetes. Since the number of cases of obesity is increasing one may want to identify these people 
quickly and reliably.

\subsection{Data Description}
\label{sec:orgc029386}

Since BMI has shown to be a bad predictor of actual fatness, this project focuses on body fat mass (BFM).
There exists very accurate methods for calculating BFM but because of high costs and efforts cheaper 
methods such as regression models are widely used. 

The given dataset (BFM MEN) describes data of body density (calculated using underwater weighing), 
age and other anthropometric variables about 252 men.

\subsection{Project Goals}
\label{sec:orgfb01111}

The main goal of the project is to create and validate our own regression model in order to predict BFM.
This includes the following:

\begin{enumerate}
\item Residual analysis for model adequacy checking
\item Handling of outliers, influential observations and leverage
\item Transformations of variables in order to correct model inadequacies
\item Multicollinearity treatments and diagnostics
\item Different types of variable selection and evaluation of these using cross validation
\item Computer-intensive procedures for model assessment (e.g. bootstrap residuals)
\end{enumerate}

\newpage
\section{Analyses and Model Development}
\label{sec:org24379ba}
\subsection{Residual analysis}
\label{sec:org9e87874}

Some major assumptions we use in our analysis are:

\begin{enumerate}
\item The errors \(\epsilon_i\) for observation \(i\) are iid. normally distributed.
\item Mean of \(\epsilon = 0\)
\item Variance of \(\epsilon = \sigma^2\), where \(\sigma\) is a constant.
\item There is approximately a linear relationship between the regressors and the response (\(y\)).
\end{enumerate}

When analysing violations of the assumptions given above, the primary tool is using the model residuals. 
We define the residual, or error, for observation \(i\) as

\[
   e_i = y_i - \hat{y_i}, \ i = 1, ... , n
   \]

One may view a residual as the difference between the data and the fit although it is also a way to analyze 
the variability in the response variable that cannot be explained by the regression model. Plotting residuals
is a effective method to examine how the regression model fits the data and make sure the assumptions listed 
are not violated.

\subsubsection{R-Student}
\label{sec:org1ca9aa7}

It is possible to use an externally studentized residual given by \cite{Montgomery2012}

\[
    t_i = \frac{e_i}{\sqrt{S^2_{i}(1 - h_{ii})}}, \ i = 1, ..., n
    \]

which is often called R-student. Here an estimate of \(\sigma^2\) is used instead of \(MS_{Res}\)
in order to create an externally studentized residual.

Now we introduce some basic residual plots, which are commonly generated using computers. These
should be analyzed routinely when solving any kind of regression modelling problem. Note that the
externally studentized residuals are often the ones plotted since they have constant variance.

\subsubsection{Normality of residuals}
\label{sec:org77cde1b}

This is a tool for analysing if two datasets (of quantiles) come from the same probability distribution. 
By plotting the quantiles against each other we will hopefully see somewhat of a straight line. This 
corresponds to them originating from the same distribution. 

Here some small departures from the normality assumption does not have a large impact. Meanwhile 
large nonnormality could have more impact because, for example, prediction intervals depend on the 
normality assumption. One may check the normality assumption simple by constructing a normal probability
plot of the residuals. 

The normality of residuals therefore ensures that the confidence intervals presented in section \ref{sec:org579dfc8}
are valid.

\subsubsection{Fitted Against Residuals}
\label{sec:org1c8facc}

Simply a plot of the, often externally studentized, residuals versus the fitted values. This is useful
because it allows an easy way to detect model inadequacies. If the plot shows the residuals contained in
a horizontal band, then the model does not contain any obvious defects. If this is not the case one may
conclude that there are likely model imperfections.
\subsubsection{Added Variable Analysis}
\label{sec:orgdc5793f}

Particularly useful when analysing if the relationship between the regressor variables and the response
has been defined accurately. Another way to use these plots are when evaluating the marginal usefulness
of some variable that is not presently a part of the model. Here \(y\) (the response variable) and \(x_j\)
(regressor) is regressed against the regressors (currently present in the model) and the residuals that
follow for each regression. When plotting these residuals against each other one may analyse the marginal
relationship for the regressor \(x_j\) that has caught our attention.

\subsubsection{Other useful plots}
\label{sec:orgfca1083}

One may want to analyze the possibility of multicollinearity being present in the data. Knowing that
this can disturb the least-squares fit in ways that results in the regression model ending up being
nearly useless. One way to do this is by create a scatter-plot of two regressors against each other
(i.e. analyzing the relationship between regressor variables. If two regressors are correlated one 
may not need to include them both in the model. If they are highly correlated the mentioned possibility 
of multicollinearity is larger. 

\subsection{Diagnostics and handling of Outliers}
\label{sec:org52d1b91}
\subsubsection{Treatment of outliers}
\label{sec:org84c7417}

An observation that is noticeably different from the rest of the data is considered an outlier. A way
to spot y space outliers is simply by analyzing the residuals. The ones that are noticeably larger 
(when considering the absolute value of these residuals) than the other residuals is an indication of
potential outliers. The magnitude of the impact caused by these outliers depends on their location
in x space. An example of identifying potential outliers is by using scaled residuals (e.g. R-student). 

Note that outliers that are considered bad values should preferably be discarded. Meanwhile there should
always be non-statistical confirmation that the outlier really is a bad value before discarding it. One
could argue that outliers are the most important part of the data since it often control many 
properties when modelling. 

One way to analyse the effect of each outliers is by simply not including the data point and refitting.
In general we prefer it when the model is not too sensitive to a small number of observations. 
Each element \(h_{ij}\) corresponds to the amount of leverage exercised by the ith observation \(y_i\) on
the jth, fitted value, \(\hat{y_j}\).

The hat matrix is can be very useful when detecting potential outliers, since it determines the variances
and covariances of \(\hat{y}\) and e. 

It appears that large hat diagonals may correspond to an influential outlier since they are remote
in x space when compared to the rest of the data. Knowing this analysts also want to observe
the studentized residuals of each observation. Large hat diagonals along with large residuals 
are likely an influential observation. 


\subsubsection{Cook's Distance}
\label{sec:org9733eb9}

One way to both of these at the same time is by using the squared distance between the least-squares
estimate (based on all n points) and also the estimate obtained when deleting the ith point. This is
called Cook’s distance and can be interpreted as the euclidean distance that the vector containing fitted
values is moved when deleting the ith observation.

\subsubsection{DFFITS \& DFBETAS}
\label{sec:org9fa8bbe}

Two other measures of the effects when deletion an observation is \(DFBETAS\) and \(DFFITS\). \(DFBETAS\) tells us
about the effects on the regression coefficient \$\hat{\beta_j} when deleting the ith observation. It is defined as
follows and is given in units of standard deviation.

\(DFFITS\) analyses the effects on the fitted value when deleting the ith observation. Here \(DFFITS\) tells us
the number of standard deviations that the fitted value is changed by when deleting observation \(i\).

\subsection{Transformations of variables}
\label{sec:org454304e}
\subsection{Diagnostics and handling of Multicolinearity}
\label{sec:orga5f4fdb}
\newpage
\section{Results}
\label{sec:org579dfc8}
\subsection{Significance tests}
\label{sec:orgf7dcff4}

Table 1 presents the ANOVA table for the full model. 

\input{../anova.tex}

\subsection{Residual analysis}
\label{sec:org5d42e3d}
\subsubsection{Normality of residuals}
\label{sec:orge6e76a5}

Figure \ref{fig:org3a41ca1} illustrates QQ plot of the model residuals. The observer may say that the 
points exhibit a pattern that indicates that the residuals come from a distribution with heavier tails
than that of a normal distribution. 
\cite{Montgomery2012}. Still, the deviations from the diagonal line is relatively small, and hence
we conclude that the first Gauss-Markov condition is fulfilled. That is, the model errors seem to be 
normally distributed.

\begin{figure}[htbp]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/qqplot.png}
\caption{\label{fig:org3a41ca1}
Normality plot of residuals.}
\end{figure}

\subsubsection{Fitted Against Residuals}
\label{sec:orgde5b18c}

Figure \ref{fig:org520653e} illustrates the fitted values \(\hat y_j\) against the R-student residuals. No apparent 
pattern is formed by the points, i.e. the points seem to be randomly scattered along the horizontal line.
Hence we conclude that the second Gauss-Markov condition is fulfilled, that is the errors have a constant 
variance.

\begin{figure}[htbp]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/far.png}
\caption{\label{fig:org520653e}
Fitted values against R-student residuals.}
\end{figure}

\subsubsection{Added Variable Analysis}
\label{sec:orge44bbf3}

Partial regression plots are found in figure \ref{fig:org7626078}, \ref{fig:org07c268e},
\ref{fig:org2f7b010}, and \ref{fig:org35c8e20}. All figures exhibits potential outliers 
(which will be further considered in section \ref{sec:org52d1b91}).
More specifically, in figure \ref{fig:org7626078} we note a 
few potential outliers on the right hand side of the plot for the \texttt{biceps} regressor, and on the
right and left hand side for the \texttt{forearm} regressor. Moreover, in figure \ref{fig:org07c268e}, we 
notice outliers on the right hand side of the \texttt{ankle} plot, and a group of potential outliers on the
\texttt{thigh} plot. Finally, we notice a few potential outliers in figure \ref{fig:org2f7b010} and 
\ref{fig:org35c8e20}.

Figure \ref{fig:org07c268e}, \ref{fig:org2f7b010}, and \ref{fig:org35c8e20} 
conveys important information about the information that \texttt{knee}, \texttt{height}, and
\texttt{chest} adds to the model. These regressors seem to follow a horizontal band along a fitted 
line from the origin, which may suggest that none of the regressors adds additional information 
to the predictions.

\begin{figure}[htbp]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/biceps_forearm_wrist_av.png}
\caption{\label{fig:org7626078}
Partial regression plots of regressors \texttt{biceps}, \texttt{forearm}, and \texttt{wrist}.}
\end{figure}   

\begin{figure}[htbp]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/thigh_knee_ankle_av.png}
\caption{\label{fig:org07c268e}
Partial regression plots of regressors \texttt{thigh}, \texttt{knee}, and \texttt{ankle}.}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/age_weight_height_neck_av.png}
\caption{\label{fig:org2f7b010}
Partial regression plots of regressors \texttt{age}, \texttt{weight}, \texttt{height}, and \texttt{neck}.}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/chest_abdomen_hip_av.png}
\caption{\label{fig:org35c8e20}
Partial regression plots of regressors \texttt{chest}, \texttt{abdomen}, and \texttt{hip}.}
\end{figure}
\subsection{Transformations of variables}
\label{sec:orge28dd7c}

Figure \ref{fig:orgf3b07d7} displays the values of \(\lambda\) to be used in a potential Box-Cox transformation of 
the dependent variable \texttt{density}. The \(\lambda\) that maximized the log-likelihood is 0.9 (0.7-1.1 95\% CI). 

Using \(\lambda = 0.9\) gives us the normal probability plot displayed on the right hand side in figure \ref{fig:orgf3b07d7}.
We notice that this affects the distribution of residuals by making it more light-tailed. That is, the 
the tails of the distribution are too light for the distribution to be considered normal.

In section \ref{sec:org9e87874} we noted that there was no indication that a transformation was needed. 
Here, we see that the transformation of the response variable only makes matters worse.

\begin{figure}[h]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/boxcox_fit.png}
\caption{\label{fig:orgf3b07d7}
Values for lambda against the log-likelihood of \texttt{density} for Box-Cox transformations.}
\end{figure}

\subsection{Diagnostics and handling of Outliers}
\label{sec:org27cf2d9}

Figure \ref{fig:org0a07a8c} illustrates Cook's distance for all points, where the three observations with the largest 
Cook's distance are labelled. Considering the cut-off \(D_i = 1\) as proposed in \cite{Montgomery2012}, 
where \(D_i\) is the Cook's distance for observation \(i\), we note that none of the observations would be 
considered influential. Still, observation 39 and 83 are largely different relative
to the other points in terms of their Cook's distance. 

\begin{figure}[h]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/cd.png}
\caption{\label{fig:org0a07a8c}
Cook's distance for all observations.}
\end{figure}

Figure \ref{fig:orgfc053b9} reports the \(DFFITS\) values. We label observations as in figure \ref{fig:org0a07a8c}. We observe 
that the three largest absolute \(DFFITS\) correspond to the same observations as in the Cook's distance plot.
The recommended cutoff-value referred to in \cite{Montgomery2012}, i.e. \(2\sqrt{\frac{p}{n}}\)
where \(p = 13\) is the number of potential regressors and \(n = 248\) is the sample size, is 
plotted as a dotted line, and the points that lie below or above this cut-off value is labelled.
We observe that several points are considered influential points when using that cut-off value.

Figure \ref{fig:org5d62bdb}, \ref{fig:org9dd4173}, \ref{fig:orgccc20de}, and
\ref{fig:org3b234c1} presents \(DFBETA\) values for groups of regressors. Observation 39
is present in a number of these figures. Using the aforementioned cut-off value of \(\frac{2}{\sqrt{n}}\), we 
we note that none of these points would be considered influential points.
\begin{figure}[h]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/dffits.png}
\caption{\label{fig:orgfc053b9}
\(DFFITS\) for all observations.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/biceps_forearm_wrist_dfbeta.png}
\caption{\label{fig:org5d62bdb}
\(DFBETA\) for regressors \texttt{biceps}, \texttt{forearm}, and \texttt{wrist}.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/age_weight_height_neck_dfbeta.png}
\caption{\label{fig:org9dd4173}
\(DFBETA\) for regressors \texttt{age}, \texttt{weight}, \texttt{height} and \texttt{neck}.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/thigh_knee_ankle_dfbeta.png}
\caption{\label{fig:orgccc20de}
\(DFBETA\) for regressors \texttt{thigh}, \texttt{knee}, and \texttt{ankle}.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/chest_abdomen_hip_dfbeta.png}
\caption{\label{fig:org3b234c1}
\(DFBETA\) for regressors \texttt{chest}, \texttt{abdomen}, and \texttt{hip}.}
\end{figure}

\subsection{Diagnostics and Handling of Multicolinearity}
\label{sec:org2f1221e}

Table 2 presents the VIF and eigen value for each respective regressor. The eigen values for the \texttt{biceps},
\texttt{forearm}, and \texttt{wrist} regressors are relatively close to zero, and the VIF of the \texttt{weight},
\texttt{chest}, \texttt{abdomen}, and \texttt{hip} regressors are larger than 10 (NOTERA DETTA I METOD). Hence, there appears 
to be an indication of multicolinearity amongst the candidate regressors.

A coorelation matrix for the full model is found in section \ref{sec:orgd44d782}. The strong multicolinearity
associated with the \texttt{weight} regressor is apparent in the correlation matrix in figure
\ref{fig:org5ec511c}. The \texttt{weight} regressor shows a strong correlation with all but the \texttt{age} and
the \texttt{height} regressors.

In a later stage we use all possible regression in order to determine the candidate regression models. 
In that stage we further examine the results of removing the regressors with an indication of high 
multicolinearity.

\input{../mc.tex} 

\subsection{Variable selection}
\label{sec:orgdaf394e}

The full results of the best subsets regression is presented in section \ref{sec:orgfa5c2c3}. The model corresponding
to each model index in Table 3 in \ref{sec:orgfa5c2c3} is presented in Table 4. The best subset plots 
are presented in figure \ref{fig:org2712770}. 

\begin{figure}[htbp]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/variable_selection.png}
\caption{\label{fig:org2712770}
Number of regressors against multiple performance measures for the best subset regression models.}
\end{figure}

\newpage
\section{Conclusion}
\label{sec:org0d1bef2}
\bibliographystyle{plain}
\bibliography{library}
\section{Appendix A}
\label{sec:orgd44d782}
\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{/home/ludvigwgerdin/courses/Regression Analysis/regone/hm.png}
\caption{\label{fig:org5ec511c}
Correlation matrix of the full model}
\end{figure}

\section{Appendix B}
\label{sec:orgfa5c2c3}

\input{../variable_selection.tex}

\input{../model_table.tex}
\end{document}