#+OPTIONS: toc:nil
#+LATEX_HEADER: \usepackage[margin=1.25in]{geometry} \usepackage{booktabs} \usepackage{graphicx} \usepackage{adjustbox} \usepackage{amsmath} \hypersetup{colorlinks=true,linkcolor=blue} \usepackage{amsthm} \newtheorem{definition}{Definition} \usepackage{bookmark}
\begin{titlepage}
\centering
\includegraphics[width=0.15\textwidth]{example-image-1x1}\par\vspace{1cm}
{\scshape\LARGE Kungliga Tekniska Högskolan \par}
\vspace{1cm}
{\scshape\Large SF2930 Regression Analysis \par}
\vspace{1.5cm}
{\huge\bfseries Report I \\  \par}
\vspace{2cm}
{\Large\itshape Isac Karlsson\\ Ludvig Wärnberg Gerdin}
\vfill
Examiner \par
\textsc{Tatjana Pavlenko}

\vfill

{\large \today\par}
\end{titlepage}
# Page break
\newpage
\tableofcontents
\newpage

* Introduction and Project Goals
** Introduction
  Our choice of scenario is Scenario I: Body fat assessment, which involves Large-Sample regression (p < n). 
  According to the World Health organization (WHO) obesity, the state where excess body fat is causing
  extensive health effects, is a large risk factor for some chronic diseases. Some examples are cancer
  and diabetes. Since the number of cases of obesity is increasing one may want to identify these people 
  quickly and reliably.

** Data Description

   Since BMI has shown to be a bad predictor of actual fatness, this project focuses on body fat mass (BFM).
   There exists very accurate methods for calculating BFM but because of high costs and efforts cheaper 
   methods such as regression models are widely used. 

   The given dataset (BFM MEN) describes data of body density (calculated using underwater weighing), 
   age and other anthropometric variables about 252 men.

** Project Goals 

  The main goal of the project is to create and validate our own regression model in order to predict BFM.
  This includes the following:

  1. Residual analysis for model adequacy checking
  2. Handling of outliers, influential observations and leverage
  3. Transformations of variables in order to correct model inadequacies
  4. Multicollinearity treatments and diagnostics
  5. Different types of variable selection and evaluation of these using cross validation
  6. Computer-intensive procedures for model assessment (e.g. bootstrap residuals)

\newpage
* Analyses and Model Development
  
  The information presented in the proceding sections are primarily taken from \textit{Introduction to
  Linear Regression Analysis} \cite{Montgomery2012}. If not, the information is cited.

** Residual analysis

   Some major assumptions we use in our analysis are:

   1. The errors $\epsilon_i$ for observation $i$ are independently and identically normally distributed.
   2. Mean of $\epsilon = 0$
   3. Variance of $\epsilon = \sigma^2$, where $\sigma$ is a constant.
   4. There is approximately a linear relationship between the regressors and the response ($y$).

   When analysing violations of the assumptions given above, the primary tool is using the model residuals. 
   We define the residual for observation $i$ as
   
   \[
   e_i = y_i - \hat{y_i}, \ i = 1, ... , n
   \]

   One may view a residual as the difference between the data and the fit although it is also a way to analyze 
   the variability in the response variable that cannot be explained by the regression model. Plotting residuals
   is a effective method to examine how the regression model fits the data and make sure the assumptions listed 
   are not violated.

*** R-Student

    One type of residual is the externally studentized residual, which is given by

    \[
    t_i = \frac{e_i}{\sqrt{S^2_{i}(1 - h_{ii})}}, \ i = 1, ..., n
    \]

    The externally studentized residual is also called the R-student residual. 
    Here an estimate of $\sigma^2$ is used instead of $MS_{Res}$ in order to create an 
    \textit{externally} studentized residual.

    Now we introduce some basic residual plots, which are commonly generated using computers. These
    should be analyzed routinely when solving any kind of regression modelling problem. Note that the
    externally studentized residuals are often the ones plotted since they have constant variance.

*** Normality of residuals

    This is a tool for analysing if two datasets (of quantiles) come from the same probability distribution. 
    By plotting the quantiles against each other we will hopefully see somewhat of a straight line. This 
    corresponds to them originating from the same distribution. 

    Here some small departures from the normality assumption does not have a large impact. Meanwhile 
    large nonnormality could have more of an impact on the regression modelling process. Mainly, the problems 
    relate to inference of the model building - for example, prediction intervals depend on the 
    normality assumption. One may check the normality assumption simply by constructing a normal probability
    plot of the residuals. 
    
*** Fitted Values Against Residuals 

    Simply a plot of the, often externally studentized, residuals versus the fitted values. This is useful
    because it allows an easy way to detect model inadequacies. If the plot shows the residuals contained in
    a horizontal band, then the model does not contain any obvious defects. If this is not the case one may
    conclude that there are likely model imperfections.
    
*** Added Variable Analysis

    Particularly useful when analysing if the relationship between the regressor variables and the response
    has been defined accurately. Another way to use these plots are when evaluating the marginal usefulness
    of some variable that is not presently a part of the model. Here $y$ (the response variable) and $x_j$
    (regressor) is regressed against the regressors (currently present in the model) and the residuals that
    follow for each regression. When plotting these residuals against each other one may analyse the marginal
    relationship for the regressor $x_j$ that has caught our attention.

** Diagnostics and handling of Outliers
*** Treatment of outliers

    An observation that is noticeably different from the rest of the data is considered an outlier. A way
    to spot y space outliers is simply by analyzing the residuals. The ones that are noticeably larger 
    (when considering the absolute value of these residuals) than the other residuals is an indication of
    potential outliers. The magnitude of the impact caused by these outliers depends on their location
    in x space. An example of identifying potential outliers is by using scaled residuals (e.g. R-student). 

    Note that outliers that are considered bad values, e.g. values from mis-measuresments,
    should preferably be discarded. Meanwhile there should
    always be non-statistical confirmation that the outlier really is a bad value before discarding it. One
    could argue that outliers are the most important part of the data since it often control many 
    properties when modelling. 

    One way to analyse the effect of each outliers is by simply not including the data point and refitting.
    In general we prefer it when the model is not too sensitive to a small number of observations. 

    The hat matrix is can be very useful when detecting potential outliers, since it determines the variances
    and covariances of $\hat{y}_j$ and $\textbf{e}$. Each element $h_{ij}$ corresponds to the amount of
    leverage exercised by the ith observation $y_i$ on the jth, fitted value, $\hat{y_j}$.

    It appears that large hat diagonals may correspond to an influential outlier since they are remote
    in x space when compared to the rest of the data. Knowing this analysts also want to observe
    the studentized residuals of each observation. Large hat diagonals along with large residuals 
    are likely an influential observation. 

*** Cook's Distance

    One way to both of these at the same time is by using the squared distance between the least-squares
    estimate (based on all n points) and also the estimate obtained when deleting the ith point. This is
    called Cook’s distance and can be interpreted as the euclidean distance that the vector containing fitted
    values is moved when deleting the ith observation. 

    The Cook's distance is arguably one of the more important metrics for our prediction purpose, since is highlight's
    the observation's effect on the predicted y-values. \cite{22286}

*** DFFITS & DFBETAS

    Two other measures of the effects when deletion an observation is $DFBETAS$ and $DFFITS$. $DFBETAS$ tells us
    about the effects on the regression coefficient $\hat{\beta_j} when deleting the ith observation. It is defined as
    follows and is given in units of standard deviation.

    $DFFITS$ analyses the effects on the fitted value when deleting the ith observation. Here $DFFITS$ tells us
    the number of standard deviations that the fitted value is changed by when deleting observation $i$. Since 
    the $DFFITS$ values consider the effect on the fitted value, this metric is arguably one of the more important 
    ones for our purpose.

    $DFBETA$ is presumably more interesting from an explanatory point-of-view \cite{22286}, which is not the
    primary purpose of this report. We therefore analyse the Cook's distance and the $DFFITS$ values more
    thoroughly that the $DFBETA$ values.
    
** Transformations of variables

   Whenever an assumption mentioned above is violated it is usually a good idea to consider data transformation. 
   In some cases expressing the regressor and or the response variables using another measurement results in 
   violations no longer being present, e.g. inequality of variance. 

   If we wish to transform y, in order to correct for example nonconstant variance, we can use the power
   transformation ylambda where lambda is what we want to determine. We can do this by using the Box-Cox method
   which also allows us to estimate the parameters of the regression model simultaneously, using maximum likelihood.
   The method is described as follows:

   Note that when analysing a partial regression plot for some regressor variable x1 ,entering the model linearly, 
   then partial residuals will show a straight line. Note that the slope of this line is the regression coefficient 
   of x1 in the multiple regression model. When x1 is considered a candidate variable for the model, if the partial
   regression plot shows a horizontal band, that tells us that no additional information for predicting y is 
   described by x1. When the partial regression plot shows a curvilinear band, then one may use a transformation 
   (e.g. replacing x1 with 1/x1).

** Diagnostics and handling of Multicolinearity
   
   Note that if the equation given above is approximately true, at least for some subset of the columns of X,
   then the problem of multicollinearity exists. As a result of this the least-squares analysis, of the model
   itself, may be very deficient. This may cause the usefulness of the regression model to decrease significantly. 

   One simple way to detect multicollinearity is by inspecting the off-diagonal element rij in X’X. A near
   linear dependency between xi and xj will result in abs(rij) to be near unity. Note that this is useful for
   detecting linear dependence between pairs of regressors and that this can not be used as a tools for
   detecting anything more complex than that. Therefore, this method of detecting multicolinearity will
   only be considered as a complementary method to more appropriate methods described here.

   The diagonal elements of the matrix C = (X’X)-1 can also be used for detecting multicollinearity. Note that 
   the jth element of C can be written as follows: Cjj=(1-Rj2)-1, 
   here R^2j is obtained when xj is regressed on the other p-1 regressors.
   When xj is almost orthogonal to the other regressors, Rj2 is small and Cjj is close to unity. Meanwhile 
   if xj is nearly linear dependent, on a subset of the other regressors, R2j is close to unity and Cjj is large.

   One may also analyze the characteristic roots/eigenvalues of X’X to measure the extent of multicollinearity. 
   When one or more of the eigenvalues are small, then there exists one or more near-linear dependencies. 
   The condition number of X’X defined as:

   if <100, no serious problem
   if between 100-1000, medium multicollinearity
   if >1000, strong multicollinearity

   As an ending note, we should mention the inhererent multicolinearity in this dataset. Most candidate predictors 
   are measures of body size, which naturally causes the predictors to be linearly related in to each other. That 
   being said, it is still appropriate to investigate methods to alleviate the effect of multicolinearity since 
   the stability of the model is heavily influenced by it. 

** Computer-Intensive Procedures and Variable Selection
*** The Boostrap
   Bootstrapping is a computer-intensive technique that allow us to compute reliable estimates of the standard
   errors of regression estimates when there is no standard procedure available or cases where the results are
   only approximate techniques (e.g. based on large-sample theory). 

   If we are interested in a particular regression coefficient BetaHat. First we are required to select a random
   sample of size n with replacement from this original sample, this is called the bootstrap sample. Then we
   proceed to fit the model to this sample by using the procedure as for the original sample. This gives us
   the first bootstrap estimate BetaHat1(star). We repeat this process many times and each repetition, a new 
   bootstrap sample is selected, the model is fit, and an estimate BetaHati(star) is concluded. 
*** Variable selection
   If multicollinearity is present, variable selection methods are very useful. Note that variable selection does
   not result in complete elimination of multicollinearity, in some cases two or more regressors are highly related 
   even though some subset of them indeed should be a part of the model, instead it helps us justify the presence
   of multicollinearity in the final model. One should also note that experience and subjective considerations
   should always be considered as a part of the variable selection problem.

*** All Possible Regression and Other Methods

   Simply requires to fit all the regression equations starting with one candidate regressor, then two
   candidate regressors and so on. These are later analyzed regarding some criterion and the “best” one is selected. 

   Since evaluating all possible regressions can sometimes be time consuming computationally, there are other
   methods for evaluating only a smaller number of subset regression models by adding/removing regressors one
   at a time. These methods are generally called stepwise procedures, and examples are forward selection and backward
   elimination. These are not considered here, since the use of all possible regression is justified.

   Note that we have not included any of the stepwise regression methods mentioned above. Primarily
   because of the list of problems connected with these methods \cite{20856}, which are for example that they yield
   R-squared values that are highly biased and cause severe problems in the presence of collinearity.

\newpage
* Results
** Sample characteristics
   
   Table \ref{tab:tblone} reports the sample characteristics. These are left for the reader, in particular to
   compare with the outliers presented in section [[Diagnostics and Handling of Outliers]].

   \input{../main/tblone.tex}

** Residual analysis
*** Normality of residuals
    
    Figure [[fig:qqplot]] illustrates a quantile-quantile plot of the externally studentized residuals.
    The observer may say that the points exhibit a pattern that indicates that the residuals are distribute with
    heavier tails than that of a normal distribution. \cite{Montgomery2012}. Still, the deviations from the
    diagonal line is relatively small, and hence we conclude that the residuals are normally distributed.

    #+NAME: fig:qqplot
    #+CAPTION: Normality plot of residuals.
    #+ATTR_LATEX: :width 8cm
    [[~/courses/Regression Analysis/regone/main/qqplot.png]]

*** Fitted Against Residuals
    
    Figure [[fig:far]] illustrates the fitted values $\hat y_j$ against the R-student residuals. No apparent 
    pattern is formed by the points, i.e. the points seem to be randomly scattered along the dotted horizontal
    line. Hence we conclude that the residuals have constant variance, and thus assume that the errors do
    as well.

    #+NAME: fig:far
    #+CAPTION: Fitted values against R-student residuals.
    #+ATTR_LATEX: :width 8cm
    [[~/courses/Regression Analysis/regone/main/far.png]]
   
*** Added Variable Analysis

    Partial regression plots are found in figure [[fig:biceps_forearm_wrist_av]], [[fig:thigh_knee_ankle_av]],
    [[fig:age_weight_height_neck]], and [[fig:chest_abdomen_hip_av]]. All figures exhibit potential points 
    that are unusually large in the x-space and hence their influence on the model fit should be 
    examined further. This will be considered in section [[Diagnostics and handling of Outliers]].
    Interestingly, 

    #+NAME: fig:biceps_forearm_wrist_av
    #+CAPTION: Partial regression plots of regressors \texttt{biceps}, \texttt{forearm}, and \texttt{wrist}.
    #+ATTR_LATEX: :width 8cm
    [[~/courses/Regression Analysis/regone/main/biceps_forearm_wrist_av.png]]   

    #+NAME: fig:thigh_knee_ankle_av
    #+CAPTION: Partial regression plots of regressors \texttt{thigh}, \texttt{knee}, and \texttt{ankle}.
    #+ATTR_LATEX: :width 8cm
    [[~/courses/Regression Analysis/regone/main/thigh_knee_ankle_av.png]]

    #+NAME: fig:age_weight_height_neck
    #+CAPTION: Partial regression plots of regressors \texttt{age}, \texttt{weight}, \texttt{height}, and \texttt{neck}.
    #+ATTR_LATEX: :width 8cm
    [[~/courses/Regression Analysis/regone/main/age_weight_height_neck_av.png]]

    #+NAME: fig:chest_abdomen_hip_av
    #+CAPTION: Partial regression plots of regressors \texttt{chest}, \texttt{abdomen}, and \texttt{hip}.
    #+ATTR_LATEX: :width 8cm
    [[~/courses/Regression Analysis/regone/main/chest_abdomen_hip_av.png]]

** Significance tests
   
   Table \ref{tab:anova} presents the Analysis of Variance table (ANOVA) for the full model. In the 
   preceding sections we concluded that the R-student residuals seem to be randomly scattered and 
   that the R-student residuals approximately follows a normal distribution. Therefore, we assume 
   that the significance tests presented here are valid. 

   The results from the ANOVA analysis will not be covered in detail in the preceding sections. Since
   our primary purpose is prediction, not explanation, the results presented here are left for the 
   readers interpretation. Instead, we place greater emphasis on handling multicolinearity 
   (see section [[Diagnostics and Handling of Multicolinearity]]) and conducting
   cross-validation for model development (see section [[Variable selection]]),
   since these aspects affect the stability of our predictions and generalizability of our model.

   \input{../main/anova.tex}

** Transformations of variables

   In section [[Residual analysis]] we noted that there was no indication that a transformation was needed on the 
   response variable. Here, we will see that the transformation of the response variable skews the results negatively.
   Figure [[fig:boxcox_fit]] displays the values of $\lambda$ to be used in a potential Box-Cox transformation of 
   the dependent variable \texttt{density}. The $\lambda$ that maximized the log-likelihood is 0.9 
   (0.7-1.1 approximate 95% CI). Using $\lambda = 0.9$ gives us the normal probability plot displayed on the 
   right hand side in figure [[fig:boxcox_fit]]. We notice that this affects the distribution of residuals by
   making it more light-tailed. 

   #+NAME: fig:boxcox_fit
   #+CAPTION: Values for lambda against the log-likelihood for Box-Cox transformations.
   #+ATTR_LATEX: :width 8cm :placement [h]
   [[~/courses/Regression Analysis/regone/main/boxcox_fit.png]]

** Diagnostics and Handling of Multicolinearity
   
   Table \ref{tab:mc} presents the VIF for each respective regressor and eigen values of the
   $\textbf{X}\textbf{X}'$. The eigen values for the 
   \texttt{biceps}, \texttt{forearm}, and \texttt{wrist} regressors are relatively close to zero, and the
   VIF of the \texttt{weight}, \texttt{chest}, \texttt{abdomen}, and \texttt{hip} regressors are larger than 10.
   Hence, there appears to be multicolinearity in the data.

   A correlation matrix for the full model is found in section [[Appendix A]]. The strong collinearity
   between the \texttt{weight} regressor and other predictors is apparent in the correlation matrix in figure
   [[fig:hm]]. The \texttt{weight} regressor shows a strong correlation with all but the \texttt{age} and
   the \texttt{height} regressors.

   \input{../main/mc.tex} 

   In order to handle the multicollinearity in the data, we replace the variables that appear to be involved 
   in the multicolinearity with a summary variable. \cite{Montgomery2012} The summary variable is referred to as
   \texttt{combo} and was defined as $\frac{\texttt{hip}\times\texttt{thigh}\times\texttt{abdomen}}{\texttt{weight}}$
   The rationale for this particular combiation of predictors was that it minimizes the MSE, as well as makes sure
   that the VIF are below 10 and that the eigen values of the $\textbf{X}\textbf{X}'$. The resulting VIF are
   presented in figure [[fig:vif_combo]]. 

   The residual analysis were re-run in order to make sure that the assumptions for normality still hold.
   The plots are presented in [[Appendix B]]. We note that the effort to reduce multicolinearity did not affect the
   other diagnostics in a noticeable way. Therefore, we keep the summary variable and move to handling of outliers.

   #+NAME: fig:vif_combo
   #+CAPTION: Variance Inflation Factors (VIF) when using the summary variable \texttt{combo}.
   #+ATTR_LATEX: :width 8cm
   [[~/courses/Regression Analysis/regone/woinfluence/vif.png]]   

** Diagnostics and Handling of Outliers
   
   Figure [[fig:cd]] illustrates Cook's distance for all points, where the three observations with the largest 
   Cook's distance are labelled. Considering the cut-off $D_i = 1$ as proposed in \cite{Montgomery2012}, 
   where $D_i$ is the Cook's distance for observation $i$, we note that none of the observations would be 
   considered influential. Still, observation 39, 83, and 41 are large relative
   to the other points in terms of their Cook's distance, which have been mentioned as a diagnostic for further
   inspection of outliers. \cite{Fox1991} The three points are henceforth considered as outliers that may
   influence our model fit in a considerable way.

   #+NAME: fig:cd
   #+CAPTION: Cook's distance for all observations.
   #+ATTR_LATEX: :width 8cm :placement [h]
   [[~/courses/Regression Analysis/regone/combo/cd.png]]

   Figure [[fig:dffits]] reports the $DFFITS$ values. 
   The recommended cutoff-value mentioned in \cite{Montgomery2012}, i.e. $\pm 2\sqrt{\frac{p}{n}}$
   where $p = 13$ is the number of potential regressors and $n = 248$ is the sample size, is 
   plotted as a dotted line, and the points that lie below or above this cut-off value is labelled.
   We observe that several points are considered influential points when using that cut-off value.
   The same values as from the Cook's distance plot appear in the $DFFITS$ plot, but also several 
   new points

   #+NAME: fig:dffits
   #+CAPTION: $DFFITS$ for all observations.
   #+ATTR_LATEX: :width 8cm :placement [h]
   [[~/courses/Regression Analysis/regone/combo/dffits.png]]

   Figure [[fig:biceps_forearm_wrist_dfbeta]], [[fig:age_weight_height_neck_dfbeta]], [[fig:thigh_knee_ankle_dfbeta]], and
   [[fig:chest_abdomen_hip_dfbeta]] in section [[Appendix C]] presents $DFBETA$ values for groups of regressors. 
   Observation 39 is present in a number of these figures, as well as observation number 83 and 217. 
   Using the aforementioned cut-off value of $\frac{2}{\sqrt{n}}$, we note however that none of these points
   would be considered influential points.

   We present the observations noted in the Cook's distance and DFFITS plots in Table \ref{tab:influence}.
   The points labelled in the $DFBETA$ plots are not considered by the reason noted previously 
   in section [[DFFITS & DFBETAS]]. The points that was identified as potential outliers in the added-variable
   plots can be compared to the points that are considered as influential in the Cook's distance plots
   and the DFFITS plot. For example, we see that observation 39 would be noted as an outlier in a number of 
   added-variable plots, and is also in included as one of the more influential observations considering 
   its DFFITS and Cook's distance values.

   We analyse these observations from two perspectives: Cause of outlier tendencies and effect on fit of 
   the model. Looking at the observations, we note that some observations are unlikely but 
   still plausible measurements, for example observation 39. In other words, they are likely not results
   of mis-measurements, and hence should not be removed for that reason. The second perspective is handled 
   section [[Variable selection]].

   \input{../combo/influence_table.tex}

** Variable selection
   
   The measurements for BIC, the C(p) criterion, and adjusted $R^2$ of the best subset models are presented
   in figure [[fig:variable_selection]]. The most well performing model, determined by its cross-validated 
   mean squared error, its predictors and the corresponding coefficients along with 95% confidence intervals are 
   presented in Table \ref{tab:coeffs}. The cross-validated MSE for the full model, the model with a summary variable, 
   and the model the summary variable without the influential observations are presented in table
   \ref{tab:performance}. 

   Several methodological considerations were made in this step. Firstly, regarding influential and outlier 
   observations. By removing influential observations we reduce the mean squared error by a considerable amount.
   However, we have no quantitative nor qualitative reason for removing them. Therefore, we will leave the 
   outliers in the dataset. 
   
   Secondly, regarding our method of handling multicolinearity. Since our primary purpose was prediction, 
   one could argue that we should proceeded with the model that minimizes the MSE on the test sample, that is
   the full model without the summary variable. We would argue, however, that by handling multicolinerity we 
   ensure more stable least-squares estimators for the model, and hence more stable predictions. In doing so,
   we sacrifice a gain in MSE. There are other methods of handling multicolinearity that were not considered
   here, for example Principal Component Regression (PCR) or ridge regression.

   Thirdly, the choice to bootstrap confidence intervals around the model coefficients. Another method 
   would be conduct the percentile bootstrap on the prediction intervals \cite{davison_hinkley_1997}. This 
   would arguably be more useful for our purpose, since the primary use of this model would be prediction. 
   However, the CI boostrap around the regression coefficients give us a confidence estimate around 
   the coefficients of our model and is therefore useful for prediction as well.
   
   \input{../performance.tex}

   \input{../woinfluence/coeffs.tex}

   #+NAME: fig:cv_apr
   #+CAPTION: Cross-validated mean squared error for the best subset model and number of regressors.
   #+ATTR_LATEX: :width 8cm
   [[~/courses/Regression Analysis/regone/woinfluence/cv_apr.png]]
   
   #+NAME: fig:variable_selection
   #+CAPTION: Number of regressors against multiple performance measures for the best subset regression models.
   #+ATTR_LATEX: :width 8cm
   [[~/courses/Regression Analysis/regone/woinfluence/apr.png]]
  
\newpage
* Conclusion
  
  A preliminary model should include. The included predictors and the corresponding coefficients are 

  Contextual dimensions are missing for us to make an adequate decision on whether this model should be implemented
  such as the cost and context of measurement.

* Appendix A

  #+NAME: fig:hm
  #+CAPTION: Correlation matrix of the full model
  #+ATTR_LATEX: :placement [H]
  [[~/courses/Regression Analysis/regone/main/hm.png]]

  \newpage

* Appendix B
* Appendix C

   #+NAME: fig:biceps_forearm_wrist_dfbeta
   #+CAPTION: $DFBETA$ for regressors \texttt{biceps}, \texttt{forearm}, and \texttt{wrist}.
   #+ATTR_LATEX: :width 8cm :placement [H]
   [[~/courses/Regression Analysis/regone/main/biceps_forearm_wrist_dfbeta.png]]

   #+NAME: fig:thigh_knee_ankle_dfbeta
   #+CAPTION: $DFBETA$ for regressors \texttt{thigh}, \texttt{knee}, and \texttt{ankle}.
   #+ATTR_LATEX: :width 8cm :placement [H]
   [[~/courses/Regression Analysis/regone/main/thigh_knee_ankle_dfbeta.png]]

   #+NAME: fig:age_weight_height_neck_dfbeta
   #+CAPTION: $DFBETA$ for regressors \texttt{age}, \texttt{weight}, \texttt{height} and \texttt{neck}.
   #+ATTR_LATEX: :width 8cm :placement [H]
   [[~/courses/Regression Analysis/regone/main/age_weight_height_neck_dfbeta.png]]

   #+NAME: fig:chest_abdomen_hip_dfbeta
   #+CAPTION: $DFBETA$ for regressors \texttt{chest}, \texttt{abdomen}, and \texttt{hip}.
   #+ATTR_LATEX: :width 8cm :placement [H]
   [[~/courses/Regression Analysis/regone/main/chest_abdomen_hip_dfbeta.png]]

\newpage

* References

\bibliographystyle{plain}
\bibliography{library}
