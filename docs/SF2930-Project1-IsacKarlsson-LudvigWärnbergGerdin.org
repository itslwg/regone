#+OPTIONS: toc:nil
#+LATEX_HEADER: \usepackage[margin=1.25in]{geometry} \usepackage{booktabs} \usepackage{graphicx} \usepackage{adjustbox} \usepackage{amsmath} \hypersetup{colorlinks=true,linkcolor=blue} \usepackage{amsthm} \newtheorem{definition}{Definition} \usepackage{bookmark}
\begin{titlepage}
\centering
\includegraphics[width=0.15\textwidth]{example-image-1x1}\par\vspace{1cm}
{\scshape\LARGE Kungliga Tekniska Högskolan \par}
\vspace{1cm}
{\scshape\Large SF2930 Regression Analysis \par}
\vspace{1.5cm}
{\huge\bfseries Report I \\  \par}
\vspace{2cm}
{\Large\itshape Isac Karlsson\\ Ludvig Wärnberg Gerdin}
\vfill
Examiner \par
\textsc{Tatjana Pavlenko}

\vfill

{\large \today\par}
\end{titlepage}
# Page break
\newpage
\tableofcontents
\newpage

* Introduction and Project Goals
** Introduction
  Our choice of scenario is Scenario I: Body fat assessment, which involves Large-Sample regression (p < n). 
  According to the World Health organization (WHO) obesity, the state where excess body fat is causing
  extensive health effects, is a large risk factor for some chronic diseases. Some examples are cancer
  and diabetes. Since the number of cases of obesity is increasing one may want to identify these people 
  quickly and reliably.

** Data Description

   Since BMI has shown to be a bad predictor of actual fatness, this project focuses on body fat mass (BFM).
   There exists very accurate methods for calculating BFM but because of high costs and efforts cheaper 
   methods such as regression models are widely used. 

   The given dataset (BFM MEN) describes data of body density (calculated using underwater weighing), 
   age and other anthropometric variables about 252 men.

** Project Goals 

  The main goal of the project is to create and validate our own regression model in order to predict BFM.
  This includes the following:

  1. Residual analysis for model adequacy checking
  2. Handling of outliers, influential observations and leverage
  3. Transformations of variables in order to correct model inadequacies
  4. Multicollinearity treatments and diagnostics
  5. Different types of variable selection and evaluation of these using cross validation
  6. Computer-intensive procedures for model assessment (e.g. bootstrap residuals)

\newpage
* Analyses and Model Development
** Residual analysis

   Some major assumptions we use in our analysis are:

   1. The errors $\epsilon_i$ for observation $i$ are iid. normally distributed.
   2. Mean of $\epsilon = 0$
   3. Variance of $\epsilon = \sigma^2$, where $\sigma$ is a constant.
   4. There is approximately a linear relationship between the regressors and the response ($y$).

   When analysing violations of the assumptions given above, the primary tool is using the model residuals. 
   We define the residual, or error, for observation $i$ as
   
   \[
   e_i = y_i - \hat{y_i}, \ i = 1, ... , n
   \]

   One may view a residual as the difference between the data and the fit although it is also a way to analyze 
   the variability in the response variable that cannot be explained by the regression model. Plotting residuals
   is a effective method to examine how the regression model fits the data and make sure the assumptions listed 
   are not violated.

*** R-Student

    It is possible to use an externally studentized residual given by \cite{Montgomery2012}

    \[
    t_i = \frac{e_i}{\sqrt{S^2_{i}(1 - h_{ii})}}, \ i = 1, ..., n
    \]

    which is often called R-student. Here an estimate of $\sigma^2$ is used instead of $MS_{Res}$
    in order to create an externally studentized residual.

    Now we introduce some basic residual plots, which are commonly generated using computers. These
    should be analyzed routinely when solving any kind of regression modelling problem. Note that the
    externally studentized residuals are often the ones plotted since they have constant variance.

*** Normality of residuals

    This is a tool for analysing if two datasets (of quantiles) come from the same probability distribution. 
    By plotting the quantiles against each other we will hopefully see somewhat of a straight line. This 
    corresponds to them originating from the same distribution. 

    Here some small departures from the normality assumption does not have a large impact. Meanwhile 
    large nonnormality could have more impact because, for example, prediction intervals depend on the 
    normality assumption. One may check the normality assumption simple by constructing a normal probability
    plot of the residuals. 

    The normality of residuals therefore ensures that the confidence intervals presented in section [[Results]]
    are valid.
    
*** Fitted Against Residuals 

    Simply a plot of the, often externally studentized, residuals versus the fitted values. This is useful
    because it allows an easy way to detect model inadequacies. If the plot shows the residuals contained in
    a horizontal band, then the model does not contain any obvious defects. If this is not the case one may
    conclude that there are likely model imperfections.
    
*** Added Variable Analysis

    Particularly useful when analysing if the relationship between the regressor variables and the response
    has been defined accurately. Another way to use these plots are when evaluating the marginal usefulness
    of some variable that is not presently a part of the model. Here $y$ (the response variable) and $x_j$
    (regressor) is regressed against the regressors (currently present in the model) and the residuals that
    follow for each regression. When plotting these residuals against each other one may analyse the marginal
    relationship for the regressor $x_j$ that has caught our attention.

*** Other useful plots

    One may want to analyze the possibility of multicollinearity being present in the data. Knowing that
    this can disturb the least-squares fit in ways that results in the regression model ending up being
    nearly useless. One way to do this is by create a scatter-plot of two regressors against each other
    (i.e. analyzing the relationship between regressor variables. If two regressors are correlated one 
    may not need to include them both in the model. If they are highly correlated the mentioned possibility 
    of multicollinearity is larger. 

** Diagnostics and handling of Outliers
*** Treatment of outliers

    An observation that is noticeably different from the rest of the data is considered an outlier. A way
    to spot y space outliers is simply by analyzing the residuals. The ones that are noticeably larger 
    (when considering the absolute value of these residuals) than the other residuals is an indication of
    potential outliers. The magnitude of the impact caused by these outliers depends on their location
    in x space. An example of identifying potential outliers is by using scaled residuals (e.g. R-student). 

    Note that outliers that are considered bad values should preferably be discarded. Meanwhile there should
    always be non-statistical confirmation that the outlier really is a bad value before discarding it. One
    could argue that outliers are the most important part of the data since it often control many 
    properties when modelling. 

    One way to analyse the effect of each outliers is by simply not including the data point and refitting.
    In general we prefer it when the model is not too sensitive to a small number of observations. 
    Each element $h_{ij}$ corresponds to the amount of leverage exercised by the ith observation $y_i$ on
    the jth, fitted value, $\hat{y_j}$.

    The hat matrix is can be very useful when detecting potential outliers, since it determines the variances
    and covariances of $\hat{y}$ and e. 

    It appears that large hat diagonals may correspond to an influential outlier since they are remote
    in x space when compared to the rest of the data. Knowing this analysts also want to observe
    the studentized residuals of each observation. Large hat diagonals along with large residuals 
    are likely an influential observation. 


*** Cook's Distance

    One way to both of these at the same time is by using the squared distance between the least-squares
    estimate (based on all n points) and also the estimate obtained when deleting the ith point. This is
    called Cook’s distance and can be interpreted as the euclidean distance that the vector containing fitted
    values is moved when deleting the ith observation. 

    The Cook's distance is arguably one of the more important metrics for our prediction purpose, since is highlight's
    the observation's effect on the predicted y-values. \cite{22286}

*** DFFITS & DFBETAS

    Two other measures of the effects when deletion an observation is $DFBETAS$ and $DFFITS$. $DFBETAS$ tells us
    about the effects on the regression coefficient $\hat{\beta_j} when deleting the ith observation. It is defined as
    follows and is given in units of standard deviation.

    $DFFITS$ analyses the effects on the fitted value when deleting the ith observation. Here $DFFITS$ tells us
    the number of standard deviations that the fitted value is changed by when deleting observation $i$. Since 
    the $DFFITS$ values consider the effect on the fitted value, this metric is arguably one of the more important 
    ones for our purpose.

    $DFBETA$ is presumably more interesting from an explanatory point-of-view \cite{22286}, which is not the
    primary purpose of this report. We therefore analyse the Cook's distance and the $DFFITS$ values more
    thoroughly that the $DFBETA$ values.
    
** Transformations of variables
** Diagnostics and handling of Multicolinearity
\newpage
* Results
** Sample characteristics and significance tests 
   
   Table \ref{tab:tblone} reports the sample characteristics. These characteristics will be interesting later
   when comparing to the outliers presented in section [[Diagnostics and Handling of Outliers]].
   
   Table \ref{tab:anova} presents the Analysis of Variance table (ANOVA) for the full model. Using a
   5% significance level, we see that neither of the predictors \texttt{hip}, \texttt{knee}, \texttt{ankle},
   \texttt{biceps}, and \texttt{forearm} are significant. This indicates that the predictors should be 
   further examined in order to determine whether they should be included in the model. 

   \input{../main/tblone.tex}
   
   \input{../main/anova.tex}

** Residual analysis
*** Normality of residuals
    
    Figure [[fig:qqplot]] illustrates a quantile-quantile plot of the externally studentized residuals.
    The observer may say that the  points exhibit a pattern that indicates that the residuals come from
    a distribution with heavier tails than that of a normal distribution. 
    \cite{Montgomery2012}. Still, the deviations from the diagonal line is relatively small, and hence
    we conclude that the residuals are normally distributed.

    #+NAME: fig:qqplot
    #+CAPTION: Normality plot of residuals.
    #+ATTR_LATEX: :width 8cm
    [[~/courses/Regression Analysis/regone/main/qqplot.png]]

*** Fitted Against Residuals
    
    Figure [[fig:far]] illustrates the fitted values $\hat y_j$ against the R-student residuals. No apparent 
    pattern is formed by the points, i.e. the points seem to be randomly scattered along the horizontal line.
    Hence we conclude that the errors have constant variance.

    #+NAME: fig:far
    #+CAPTION: Fitted values against R-student residuals.
    #+ATTR_LATEX: :width 8cm
    [[~/courses/Regression Analysis/regone/main/far.png]]
   
*** Added Variable Analysis

    Partial regression plots are found in figure [[fig:biceps_forearm_wrist_av]], [[fig:thigh_knee_ankle_av]],
    [[fig:age_weight_height_neck]], and [[fig:chest_abdomen_hip_av]]. All figures exhibit potential points 
    that aren't adjacent to majority of the observations and hence their influence on the model fit should be 
    examined further. This will be considered in section [[Diagnostics and handling of Outliers]].
  
    Figure [[fig:age_weight_height_neck]], and [[fig:chest_abdomen_hip_av]] 
    convey important information about the \texttt{height}, and \texttt{chest} predictors.
    The \texttt{height} regressors exhibit a double-bow pattern, indicating that a transformation on 
    the height regressor could be suitable \cite{Montgomery2012}. This is adjusted for in the upcoming section.

    The \texttt{chest} regressor follows  a horizontal band, suggesting that the predictor does not add any
    further information to the model. \cite{Montgomery2012} This will be further considered in upcoming sections.

    #+NAME: fig:biceps_forearm_wrist_av
    #+CAPTION: Partial regression plots of regressors \texttt{biceps}, \texttt{forearm}, and \texttt{wrist}.
    #+ATTR_LATEX: :width 8cm
    [[~/courses/Regression Analysis/regone/main/biceps_forearm_wrist_av.png]]   

    #+NAME: fig:thigh_knee_ankle_av
    #+CAPTION: Partial regression plots of regressors \texttt{thigh}, \texttt{knee}, and \texttt{ankle}.
    #+ATTR_LATEX: :width 8cm
    [[~/courses/Regression Analysis/regone/main/thigh_knee_ankle_av.png]]

    #+NAME: fig:age_weight_height_neck
    #+CAPTION: Partial regression plots of regressors \texttt{age}, \texttt{weight}, \texttt{height}, and \texttt{neck}.
    #+ATTR_LATEX: :width 8cm
    [[~/courses/Regression Analysis/regone/main/age_weight_height_neck_av.png]]

    #+NAME: fig:chest_abdomen_hip_av
    #+CAPTION: Partial regression plots of regressors \texttt{chest}, \texttt{abdomen}, and \texttt{hip}.
    #+ATTR_LATEX: :width 8cm
    [[~/courses/Regression Analysis/regone/main/chest_abdomen_hip_av.png]]

** Transformations of variables

   In section [[Residual analysis]] we noted that there was no indication that a transformation was needed on the 
   response variable. Here, we see that the transformation of the response variable skews the results negatively.
   Figure [[fig:boxcox_fit]] displays the values of $\lambda$ to be used in a potential Box-Cox transformation of 
   the dependent variable \texttt{density}. The $\lambda$ that maximized the log-likelihood is 0.9 (0.7-1.1 95% CI). 

   Using $\lambda = 0.9$ gives us the normal probability plot displayed on the right hand side in figure [[fig:boxcox_fit]].
   We notice that this affects the distribution of residuals by making it more light-tailed. That is, the 
   the tails of the distribution are too light for the distribution to be considered normal.

   In section [[Residual analysis]], however, we noted that the relationship between the \texttt{height} regressor 
   and the reponse variable was misspecified as indicated by the partial regression plot. In order to correct 
   this we specify the \texttt{height} regressor as $1/\text{\texttt{height}}^2$. The resulting added-variable
   plot is shown in figure [[fig:av_height]].

   #+NAME: fig:boxcox_fit
   #+CAPTION: Values for lambda against the log-likelihood of \texttt{density} for Box-Cox transformations.
   #+ATTR_LATEX: :width 8cm :placement [h]
   [[~/courses/Regression Analysis/regone/main/boxcox_fit.png]]

   #+NAME: fig:av_height
   #+CAPTION: The partial regression plot for $1/\text{\texttt{height}}^2$
   #+ATTR_LATEX: :width 8cm :placement [h]
   [[~/courses/Regression Analysis/regone/main/av_height.png]]

** Diagnostics and Handling of Outliers
   
   Figure [[fig:cd]] illustrates Cook's distance for all points, where the three observations with the largest 
   Cook's distance are labelled. Considering the cut-off $D_i = 1$ as proposed in \cite{Montgomery2012}, 
   where $D_i$ is the Cook's distance for observation $i$, we note that none of the observations would be 
   considered influential. Still, observation 39 and 83 are largely different relative
   to the other points in terms of their Cook's distance, which have been mentioned as a diagnostic for further
   inspection of outliers. \cite{Fox1991} The three points that have the largest Cook's distance are labelled
   in the plot, and are henceforth considered as outliers that may influence our model fit in a considerable way.

   #+NAME: fig:cd
   #+CAPTION: Cook's distance for all observations.
   #+ATTR_LATEX: :width 8cm :placement [h]
   [[~/courses/Regression Analysis/regone/main/cd.png]]

   Figure [[fig:dffits]] reports the $DFFITS$ values. We label observations as in figure [[fig:cd]]. We observe 
   that the three largest absolute $DFFITS$ correspond to the same observations as in the figure [[fig:cd]]. 
   The recommended cutoff-value mentioned in \cite{Montgomery2012}, i.e. $\pm 2\sqrt{\frac{p}{n}}$
   where $p = 13$ is the number of potential regressors and $n = 248$ is the sample size, is 
   plotted as a dotted line, and the points that lie below or above this cut-off value is labelled.
   We observe that several points are considered influential points when using that cut-off value.

   #+NAME: fig:dffits
   #+CAPTION: $DFFITS$ for all observations.
   #+ATTR_LATEX: :width 8cm :placement [h]
   [[~/courses/Regression Analysis/regone/main/dffits.png]]

   Figure [[fig:biceps_forearm_wrist_dfbeta]], [[fig:age_weight_height_neck_dfbeta]], [[fig:thigh_knee_ankle_dfbeta]], and
   [[fig:chest_abdomen_hip_dfbeta]] in section [[Appendix C]] presents $DFBETA$ values for groups of regressors. 
   Observation 39 is present in a number of these figures, as well as observation number 83 and 217. 
   Using the aforementioned cut-off value of $\frac{2}{\sqrt{n}}$, we note however that none of these points
   would be considered influential points.

   We present the observations noted in the Cook's distance and DFFITS plots in Table \ref{tab:influence}.
   The points labelled in the $DFBETA$ plots are not considered by the reason noted previously 
   in section [[DFFITS & DFBETAS]]. Looking at the observations, we note some observations are unlikely but 
   plausible measurements, for example observation 39. In other words, they are not a result of mis-measurement.
   By this argument, we choose not to exclude the observations. Still, they have considerable effect on the model fit

   \input{../main/influence_table.tex}

** Diagnostics and Handling of Multicolinearity
   
   Table \ref{tab:mc} presents the VIF and eigen value for each respective regressor. The eigen values for the 
   \texttt{biceps}, \texttt{forearm}, and \texttt{wrist} regressors are relatively close to zero, and the
   VIF of the \texttt{weight}, \texttt{chest}, \texttt{abdomen}, and \texttt{hip} regressors are larger than 10.
   Hence, there appears to be multicolinearity amongst the candidate regressors.

   A coorelation matrix for the full model is found in section [[Appendix A]]. The strong multicolinearity
   associated with the \texttt{weight} regressor is apparent in the correlation matrix in figure
   [[fig:hm]]. The \texttt{weight} regressor shows a strong correlation with all but the \texttt{age} and
   the \texttt{height} regressors.

   \input{../main/mc.tex} 

** Variable selection
   
   The full results of the best subsets regression is presented in section Table \ref{tab:variable_selection}
   in [[Appendix B]]. The model corresponding to each model index in Table \ref{tab:variable_selection} is presented in
   Table \ref{tab:model_table}. The metric measurements for AIC, BIC, the C(p) criterion and adjusted $R^2$ 
   of the best subset models are presented in figure [[fig:variable_selection]]. 

   The most well performing model, determined by its cross-validated mean squared error, its predictors and
   the corresponding coefficients along with 95% confidence intervals are presented in Table \ref{tab:coeffs}.
   
   #+NAME: fig:cv_apr
   #+CAPTION: Cross-validated mean squared error for the best subset model and number of regressors.
   #+ATTR_LATEX: :width 8cm
   [[~/courses/Regression Analysis/regone/main/cv_apr.png]]
   
   #+NAME: fig:variable_selection
   #+CAPTION: Number of regressors against multiple performance measures for the best subset regression models.
   #+ATTR_LATEX: :width 8cm
   [[~/courses/Regression Analysis/regone/main/variable_selection.png]]

   \input{../main/coeffs.tex}

\newpage
* Discussion

  There are contextual dimensions that are missing in order for us to make an adequate decision on whether 
  the model should be implemented. Firstly, we do not take into the account of the cost of making the 
  measurements for prediction. The measurements themselves are easily determined.

* Conclusion

  A preliminary model should include. The included predictors and the corresponding coefficients are 

  Contextual dimensions are missing for us to make an adequate decision on whether this model should be implemented
  such as the cost and context of measurement.

* Appendix A

  #+NAME: fig:hm
  #+CAPTION: Correlation matrix of the full model
  #+ATTR_LATEX: :placement [H]
  [[~/courses/Regression Analysis/regone/main/hm.png]]

  \newpage

* Appendix B

  \input{../main/variable_selection.tex}

  \input{../main/model_table.tex}

  \newpage  

* Appendix C

   #+NAME: fig:biceps_forearm_wrist_dfbeta
   #+CAPTION: $DFBETA$ for regressors \texttt{biceps}, \texttt{forearm}, and \texttt{wrist}.
   #+ATTR_LATEX: :width 8cm :placement [H]
   [[~/courses/Regression Analysis/regone/main/biceps_forearm_wrist_dfbeta.png]]

   #+NAME: fig:thigh_knee_ankle_dfbeta
   #+CAPTION: $DFBETA$ for regressors \texttt{thigh}, \texttt{knee}, and \texttt{ankle}.
   #+ATTR_LATEX: :width 8cm :placement [H]
   [[~/courses/Regression Analysis/regone/main/thigh_knee_ankle_dfbeta.png]]

   #+NAME: fig:age_weight_height_neck_dfbeta
   #+CAPTION: $DFBETA$ for regressors \texttt{age}, \texttt{weight}, \texttt{height} and \texttt{neck}.
   #+ATTR_LATEX: :width 8cm :placement [H]
   [[~/courses/Regression Analysis/regone/main/age_weight_height_neck_dfbeta.png]]

   #+NAME: fig:chest_abdomen_hip_dfbeta
   #+CAPTION: $DFBETA$ for regressors \texttt{chest}, \texttt{abdomen}, and \texttt{hip}.
   #+ATTR_LATEX: :width 8cm :placement [H]
   [[~/courses/Regression Analysis/regone/main/chest_abdomen_hip_dfbeta.png]]

\newpage

* References

\bibliographystyle{plain}
\bibliography{library}
