% Created 2020-03-05 Thu 08:46
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\usepackage[margin=1.25in]{geometry} \usepackage{booktabs} \usepackage{graphicx} \usepackage{adjustbox} \usepackage{amsmath} \hypersetup{colorlinks=true,linkcolor=blue} \usepackage{amsthm} \newtheorem{definition}{Definition} \usepackage{bookmark}
\author{Ludde}
\date{\today}
\title{}
\hypersetup{
 pdfauthor={Ludde},
 pdftitle={},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.2 (Org mode 9.1.14)}, 
 pdflang={English}}
\begin{document}

\begin{titlepage}
\centering
\includegraphics[width=0.15\textwidth]{example-image-1x1}\par\vspace{1cm}
{\scshape\LARGE Kungliga Tekniska Högskolan \par}
\vspace{1cm}
{\scshape\Large SF2930 Regression Analysis \par}
\vspace{1.5cm}
{\huge\bfseries Report I \\  \par}
\vspace{2cm}
{\Large\itshape Isac Karlsson\\ Ludvig Wärnberg Gerdin}
\vfill
Examiner \par
\textsc{Tatjana Pavlenko}

\vfill

{\large \today\par}
\end{titlepage}

\newpage
\tableofcontents
\newpage

\section{Introduction and Project Goals}
\label{sec:orge4a1d88}
\subsection{Introduction}
\label{sec:org8d9fe52}
Our choice of scenario is Scenario I: Body fat assessment, which involves Large-Sample regression (p < n). 
According to the World Health organization (WHO) obesity, the state where excess body fat is causing
extensive health effects, is a large risk factor for some chronic diseases. Some examples are cancer
and diabetes. Since the number of cases of obesity is increasing one may want to identify these people 
quickly and reliably.

\subsection{Data Description}
\label{sec:org00d803c}

Since BMI has shown to be a bad predictor of actual fatness, this project focuses on body fat mass (BFM).
There exists very accurate methods for calculating BFM but because of high costs and efforts cheaper 
methods such as regression models are widely used. 

The given dataset (BFM MEN) describes data of body density (calculated using underwater weighing), 
age and other anthropometric variables about 252 men.

\subsection{Project Goals}
\label{sec:org2fb1324}

The main goal of the project is to create and validate our own regression model in order to predict BFM.
This includes the following:

\begin{enumerate}
\item Residual analysis for model adequacy checking
\item Handling of outliers, influential observations and leverage
\item Transformations of variables in order to correct model inadequacies
\item Multicollinearity treatments and diagnostics
\item Different types of variable selection and evaluation of these using cross validation
\item Computer-intensive procedures for model assessment (e.g. bootstrap residuals)
\end{enumerate}

\newpage
\section{Analyses and Model Development}
\label{sec:org83ebeb1}
\subsection{Residual analysis}
\label{sec:org77f8323}

Some major assumptions we use in our analysis are:

\begin{enumerate}
\item The errors \(\epsilon_i\) for observation \(i\) are iid. normally distributed.
\item Mean of \(\epsilon = 0\)
\item Variance of \(\epsilon = \sigma^2\), where \(\sigma\) is a constant.
\item There is approximately a linear relationship between the regressors and the response (\(y\)).
\end{enumerate}

When analysing violations of the assumptions given above, the primary tool is using the model residuals. 
We define the residual, or error, for observation \(i\) as

\[
   e_i = y_i - \hat{y_i}, \ i = 1, ... , n
   \]

One may view a residual as the difference between the data and the fit although it is also a way to analyze 
the variability in the response variable that cannot be explained by the regression model. Plotting residuals
is a effective method to examine how the regression model fits the data and make sure the assumptions listed 
are not violated.

\subsubsection{R-Student}
\label{sec:orge3902f6}

It is possible to use an externally studentized residual given by \cite{Montgomery2012}

\[
    t_i = \frac{e_i}{\sqrt{S^2_{i}(1 - h_{ii})}}, \ i = 1, ..., n
    \]

which is often called R-student. Here an estimate of \(\sigma^2\) is used instead of \(MS_{Res}\)
in order to create an externally studentized residual.

Now we introduce some basic residual plots, which are commonly generated using computers. These
should be analyzed routinely when solving any kind of regression modelling problem. Note that the
externally studentized residuals are often the ones plotted since they have constant variance.

\subsubsection{Normality of residuals}
\label{sec:org1bbb149}

This is a tool for analysing if two datasets (of quantiles) come from the same probability distribution. 
By plotting the quantiles against each other we will hopefully see somewhat of a straight line. This 
corresponds to them originating from the same distribution. 

Here some small departures from the normality assumption does not have a large impact. Meanwhile 
large nonnormality could have more impact because, for example, prediction intervals depend on the 
normality assumption. One may check the normality assumption simple by constructing a normal probability
plot of the residuals. 

The normality of residuals therefore ensures that the confidence intervals presented in section \ref{sec:org61466a8}
are valid.

\subsubsection{Fitted Against Residuals}
\label{sec:orgf78f7d9}

Simply a plot of the, often externally studentized, residuals versus the fitted values. This is useful
because it allows an easy way to detect model inadequacies. If the plot shows the residuals contained in
a horizontal band, then the model does not contain any obvious defects. If this is not the case one may
conclude that there are likely model imperfections.

\subsubsection{Added Variable Analysis}
\label{sec:orgd26e977}

Particularly useful when analysing if the relationship between the regressor variables and the response
has been defined accurately. Another way to use these plots are when evaluating the marginal usefulness
of some variable that is not presently a part of the model. Here \(y\) (the response variable) and \(x_j\)
(regressor) is regressed against the regressors (currently present in the model) and the residuals that
follow for each regression. When plotting these residuals against each other one may analyse the marginal
relationship for the regressor \(x_j\) that has caught our attention.

\subsubsection{Other useful plots}
\label{sec:org44d7776}

One may want to analyze the possibility of multicollinearity being present in the data. Knowing that
this can disturb the least-squares fit in ways that results in the regression model ending up being
nearly useless. One way to do this is by create a scatter-plot of two regressors against each other
(i.e. analyzing the relationship between regressor variables. If two regressors are correlated one 
may not need to include them both in the model. If they are highly correlated the mentioned possibility 
of multicollinearity is larger. 

\subsection{Diagnostics and handling of Outliers}
\label{sec:org20bf5df}
\subsubsection{Treatment of outliers}
\label{sec:orgb6f2580}

An observation that is noticeably different from the rest of the data is considered an outlier. A way
to spot y space outliers is simply by analyzing the residuals. The ones that are noticeably larger 
(when considering the absolute value of these residuals) than the other residuals is an indication of
potential outliers. The magnitude of the impact caused by these outliers depends on their location
in x space. An example of identifying potential outliers is by using scaled residuals (e.g. R-student). 

Note that outliers that are considered bad values should preferably be discarded. Meanwhile there should
always be non-statistical confirmation that the outlier really is a bad value before discarding it. One
could argue that outliers are the most important part of the data since it often control many 
properties when modelling. 

One way to analyse the effect of each outliers is by simply not including the data point and refitting.
In general we prefer it when the model is not too sensitive to a small number of observations. 
Each element \(h_{ij}\) corresponds to the amount of leverage exercised by the ith observation \(y_i\) on
the jth, fitted value, \(\hat{y_j}\).

The hat matrix is can be very useful when detecting potential outliers, since it determines the variances
and covariances of \(\hat{y}\) and e. 

It appears that large hat diagonals may correspond to an influential outlier since they are remote
in x space when compared to the rest of the data. Knowing this analysts also want to observe
the studentized residuals of each observation. Large hat diagonals along with large residuals 
are likely an influential observation. 


\subsubsection{Cook's Distance}
\label{sec:org8bee02b}

One way to both of these at the same time is by using the squared distance between the least-squares
estimate (based on all n points) and also the estimate obtained when deleting the ith point. This is
called Cook’s distance and can be interpreted as the euclidean distance that the vector containing fitted
values is moved when deleting the ith observation. 

The Cook's distance is arguably one of the more important metrics for our prediction purpose, since is highlight's
the observation's effect on the predicted y-values. \cite{22286}

\subsubsection{DFFITS \& DFBETAS}
\label{sec:orgd509730}

Two other measures of the effects when deletion an observation is \(DFBETAS\) and \(DFFITS\). \(DFBETAS\) tells us
about the effects on the regression coefficient \$\hat{\beta_j} when deleting the ith observation. It is defined as
follows and is given in units of standard deviation.

\(DFFITS\) analyses the effects on the fitted value when deleting the ith observation. Here \(DFFITS\) tells us
the number of standard deviations that the fitted value is changed by when deleting observation \(i\). Since 
the \(DFFITS\) values consider the effect on the fitted value, this metric is arguably one of the more important 
ones for our purpose.

\(DFBETA\) is presumably more interesting from an explanatory point-of-view \cite{22286}, which is not the
primary purpose of this report. We therefore analyse the Cook's distance and the \(DFFITS\) values more
thoroughly that the \(DFBETA\) values.

\subsection{Transformations of variables}
\label{sec:orga15790b}
\subsection{Diagnostics and handling of Multicolinearity}
\label{sec:org8b82377}
\newpage
\section{Results}
\label{sec:org61466a8}
\subsection{Sample characteristics}
\label{sec:org72696c0}

Table \ref{tab:tblone} reports the sample characteristics. These characteristics will be interesting later
when comparing to the outliers presented in section \ref{sec:orgae23f68}.

\input{../main/tblone.tex}

\subsection{Residual analysis}
\label{sec:orga427a9e}
\subsubsection{Normality of residuals}
\label{sec:org91640b3}

Figure \ref{fig:orgfd3e134} illustrates a quantile-quantile plot of the externally studentized residuals.
The observer may say that the  points exhibit a pattern that indicates that the residuals come from
a distribution with heavier tails than that of a normal distribution. 
\cite{Montgomery2012}. Still, the deviations from the diagonal line is relatively small, and hence
we conclude that the residuals are normally distributed.

\begin{figure}[htbp]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/qqplot.png}
\caption{\label{fig:orgfd3e134}
Normality plot of residuals.}
\end{figure}

\subsubsection{Fitted Against Residuals}
\label{sec:org8c9117a}

Figure \ref{fig:org596d012} illustrates the fitted values \(\hat y_j\) against the R-student residuals. No apparent 
pattern is formed by the points, i.e. the points seem to be randomly scattered along the horizontal line.
Hence we conclude that the errors have constant variance.

\begin{figure}[htbp]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/far.png}
\caption{\label{fig:org596d012}
Fitted values against R-student residuals.}
\end{figure}

\subsubsection{Added Variable Analysis}
\label{sec:org9606cee}

Partial regression plots are found in figure \ref{fig:orgf384fdf}, \ref{fig:org70b0661},
\ref{fig:org32af776}, and \ref{fig:org1c12d58}. All figures exhibit potential points 
that aren't adjacent to majority of the observations and hence their influence on the model fit should be 
examined further. This will be considered in section \ref{sec:org20bf5df}.

Figure \ref{fig:org32af776}, and \ref{fig:org1c12d58} 
convey important information about the \texttt{height}, and \texttt{chest} predictors.
The \texttt{height} regressors exhibit a double-bow pattern, indicating that a transformation on 
the height regressor could be suitable \cite{Montgomery2012}. This is adjusted for in the upcoming section.

The \texttt{chest} regressor follows  a horizontal band, suggesting that the predictor does not add any
further information to the model. \cite{Montgomery2012} This will be further considered in upcoming sections.

\begin{figure}[htbp]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/biceps_forearm_wrist_av.png}
\caption{\label{fig:orgf384fdf}
Partial regression plots of regressors \texttt{biceps}, \texttt{forearm}, and \texttt{wrist}.}
\end{figure}   

\begin{figure}[htbp]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/thigh_knee_ankle_av.png}
\caption{\label{fig:org70b0661}
Partial regression plots of regressors \texttt{thigh}, \texttt{knee}, and \texttt{ankle}.}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/age_weight_height_neck_av.png}
\caption{\label{fig:org32af776}
Partial regression plots of regressors \texttt{age}, \texttt{weight}, \texttt{height}, and \texttt{neck}.}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/chest_abdomen_hip_av.png}
\caption{\label{fig:org1c12d58}
Partial regression plots of regressors \texttt{chest}, \texttt{abdomen}, and \texttt{hip}.}
\end{figure}

\subsection{Significance tests}
\label{sec:org1026325}

Table \ref{tab:anova} presents the Analysis of Variance table (ANOVA) for the full model. Using a
5\% significance level, we see that neither of the predictors \texttt{hip}, \texttt{knee}, \texttt{ankle},
\texttt{biceps}, and \texttt{forearm} are significant. This indicates that the predictors should be 
further examined in order to determine whether they should be included in the model. 

\input{../main/anova.tex}

\subsection{Transformations of variables}
\label{sec:org02a8923}

In section \ref{sec:org77f8323} we noted that there was no indication that a transformation was needed on the 
response variable. Here, we will see that the transformation of the response variable skews the results negatively.
Figure \ref{fig:orgb2f9ad5} displays the values of \(\lambda\) to be used in a potential Box-Cox transformation of 
the dependent variable \texttt{density}. The \(\lambda\) that maximized the log-likelihood is 0.9 
(0.7-1.1 approximate 95\% CI). 

Using \(\lambda = 0.9\) gives us the normal probability plot displayed on the right hand side in figure
\ref{fig:orgb2f9ad5}. We notice that this affects the distribution of residuals by making it more light-tailed. 
That is, the tails of the distribution are too light for the distribution to be considered normal.

In section \ref{sec:org77f8323}, however, we noted that the relationship between the \texttt{height} regressor 
and the reponse variable was misspecified as indicated by the partial regression plot. In order to correct 
this we specify the \texttt{height} regressor as \(1/\text{\texttt{height}}^2\). The resulting added-variable
plot is shown in figure \ref{fig:org62ef8cc}.

\begin{figure}[h]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/boxcox_fit.png}
\caption{\label{fig:orgb2f9ad5}
Values for lambda against the log-likelihood of \texttt{density} for Box-Cox transformations.}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/height_av.png}
\caption{\label{fig:org62ef8cc}
The partial regression plot for \(1/\text{\texttt{height}}^2\)}
\end{figure}

\subsection{Diagnostics and Handling of Multicolinearity}
\label{sec:org01ee4af}

Table \ref{tab:mc} presents the VIF and eigen value for each respective regressor. The eigen values for the 
\texttt{biceps}, \texttt{forearm}, and \texttt{wrist} regressors are relatively close to zero, and the
VIF of the \texttt{weight}, \texttt{chest}, \texttt{abdomen}, and \texttt{hip} regressors are larger than 10.
Hence, there appears to be multicolinearity amongst the candidate regressors.

A coorelation matrix for the full model is found in section \ref{sec:org3f63af2}. The strong multicolinearity
associated with the \texttt{weight} regressor is apparent in the correlation matrix in figure
\ref{fig:org9b870c1}. The \texttt{weight} regressor shows a strong correlation with all but the \texttt{age} and
the \texttt{height} regressors.

\input{../main/mc.tex} 

In order to handle the multicollinearity in the data, we replace the involved variables with a summary variable.
When starting to replace variables, we noted that the multicolinearity shifted, resulting in a summary variable
\texttt{combo} that was defined as \(\frac{\texttt{hip}\times\texttt{thigh}\times\texttt{abdomen}}{\texttt{weight}}\)
The resulting VIF are presented in figure \ref{fig:orgdeb4a0a}. We now note that none of the VIF exceed 10.

Now that we've removed several predictors and replaced in with a summary variable, we have conduct the 
residual analysis and observe the effect on the analysis. The plots are presented in \ref{sec:orgd9225c0}. We note that 
the effort to reduce multicolinearity did not affect the other diagnostics in a noticeable way. Therefore,
we keep the summary variable and move to handling of outliers.

\begin{figure}[htbp]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/woinfluence/vif.png}
\caption{\label{fig:orgdeb4a0a}
Variance Inflation Factors (VIF) when using the summary variable \texttt{combo}.}
\end{figure}   

\subsection{Diagnostics and Handling of Outliers}
\label{sec:orgae23f68}

Figure \ref{fig:org6e551f1} illustrates Cook's distance for all points, where the three observations with the largest 
Cook's distance are labelled. Considering the cut-off \(D_i = 1\) as proposed in \cite{Montgomery2012}, 
where \(D_i\) is the Cook's distance for observation \(i\), we note that none of the observations would be 
considered influential. Still, observation 39, 83, and 41 are large relative
to the other points in terms of their Cook's distance, which have been mentioned as a diagnostic for further
inspection of outliers. \cite{Fox1991} The three points are henceforth considered as outliers that may
influence our model fit in a considerable way.

\begin{figure}[h]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/combo/cd.png}
\caption{\label{fig:org6e551f1}
Cook's distance for all observations.}
\end{figure}

Figure \ref{fig:org04d73a9} reports the \(DFFITS\) values. 
The recommended cutoff-value mentioned in \cite{Montgomery2012}, i.e. \(\pm 2\sqrt{\frac{p}{n}}\)
where \(p = 13\) is the number of potential regressors and \(n = 248\) is the sample size, is 
plotted as a dotted line, and the points that lie below or above this cut-off value is labelled.
We observe that several points are considered influential points when using that cut-off value.
The same values as from the Cook's distance plot appear in the \(DFFITS\) plot, but also several 
new points

\begin{figure}[h]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/combo/dffits.png}
\caption{\label{fig:org04d73a9}
\(DFFITS\) for all observations.}
\end{figure}

Figure \ref{fig:org8ce9f1d}, \ref{fig:org39c2a3e}, \ref{fig:orgea29e3d}, and
\ref{fig:org00853c6} in section \ref{sec:orga5afe3d} presents \(DFBETA\) values for groups of regressors. 
Observation 39 is present in a number of these figures, as well as observation number 83 and 217. 
Using the aforementioned cut-off value of \(\frac{2}{\sqrt{n}}\), we note however that none of these points
would be considered influential points.

We present the observations noted in the Cook's distance and DFFITS plots in Table \ref{tab:influence}.
The points labelled in the \(DFBETA\) plots are not considered by the reason noted previously 
in section \ref{sec:orgd509730}. 

We analyse these observations from two perspectives: Cause of outlier tendencies and effect on fit of 
the model. Looking at the observations, we note that some observations are unlikely but 
still plausible measurements, for example observation 39. In other words, they are not likely a result
of mis-measurements, and hence should not be removed for that reason. The second perspective is handled 
section \ref{sec:org968180a}.

\input{../combo/influence_table.tex}

\subsection{Variable selection}
\label{sec:org968180a}

The measurements for BIC, the C(p) criterion, and adjusted \(R^2\) of the best subset models are presented
in figure \ref{fig:orgeaf1a25}. The AIC is not included here. Still, the metric would have conveyed 
the same information regarding the most well-performing model as did the BIC. We note that 
the most well-performing model differ depending on the metric.

The most well performing model, determined by its cross-validated mean squared error, its predictors and
the corresponding coefficients along with 95\% confidence intervals are presented in Table \ref{tab:coeffs}.
The cross-validated MSE for the full model, the model with a summary variable, and the model with a
summary variable without the influential observations presented in section \ref{sec:orgae23f68}, 
is presented in table \ref{tab:performance}.

\input{../performance.tex}

\input{../woinfluence/coeffs.tex}

\begin{figure}[htbp]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/woinfluence/cv_apr.png}
\caption{\label{fig:org8f6f5df}
Cross-validated mean squared error for the best subset model and number of regressors.}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/woinfluence/apr.png}
\caption{\label{fig:orgeaf1a25}
Number of regressors against multiple performance measures for the best subset regression models.}
\end{figure}

\newpage
\section{Discussion}
\label{sec:org54d6d7b}

There are contextual dimensions that are missing in order for us to make an adequate decision on whether 
the model should be implemented. Firstly, we do not take into the account of the cost of making the 
measurements for prediction, and thus. Second, we do not know the patients that the model is supposed 
to be used on. Presumably men, but not if our sample is characterstic for the population.

Since our primary purpose was prediction, one could argue that we should proceeded with the model that minimizes the
MSE on the test sample, and not the model that handled multicolinearity better.  
We would argue, however, that by handling multicolinerity we ensure more stable least-squares estimators for 
the model, and hence more stable predictions. In doing so, we sacrifice a gain in MSE.
One could also argue that we could have handled multicolinearity in a different way, for example by conducting
Principal Component Regression (PCR), which combines variables in the direction that minimizes the variance
in the data.


\section{Conclusion}
\label{sec:org32da6ae}

A preliminary model should include. The included predictors and the corresponding coefficients are 

Contextual dimensions are missing for us to make an adequate decision on whether this model should be implemented
such as the cost and context of measurement.

\section{Appendix A}
\label{sec:org3f63af2}

\begin{figure}[H]
\centering
\includegraphics[width=.9\linewidth]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/hm.png}
\caption{\label{fig:org9b870c1}
Correlation matrix of the full model}
\end{figure}

\newpage

\section{Appendix B}
\label{sec:orgd9225c0}
\section{Appendix C}
\label{sec:orga5afe3d}

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/biceps_forearm_wrist_dfbeta.png}
\caption{\label{fig:org8ce9f1d}
\(DFBETA\) for regressors \texttt{biceps}, \texttt{forearm}, and \texttt{wrist}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/thigh_knee_ankle_dfbeta.png}
\caption{\label{fig:orgea29e3d}
\(DFBETA\) for regressors \texttt{thigh}, \texttt{knee}, and \texttt{ankle}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/age_weight_height_neck_dfbeta.png}
\caption{\label{fig:org39c2a3e}
\(DFBETA\) for regressors \texttt{age}, \texttt{weight}, \texttt{height} and \texttt{neck}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/chest_abdomen_hip_dfbeta.png}
\caption{\label{fig:org00853c6}
\(DFBETA\) for regressors \texttt{chest}, \texttt{abdomen}, and \texttt{hip}.}
\end{figure}

\newpage

\section{References}
\label{sec:orgec31821}

\bibliographystyle{plain}
\bibliography{library}
\end{document}