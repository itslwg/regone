#+OPTIONS: toc:nil
#+LATEX_HEADER: \usepackage[margin=1.25in]{geometry} \usepackage{booktabs} \usepackage{graphicx} \usepackage{adjustbox} \usepackage{amsmath} \hypersetup{colorlinks=true,linkcolor=blue} \usepackage{amsthm} \newtheorem{definition}{Definition} \usepackage{bookmark}
\begin{titlepage}
\centering
\includegraphics[width=0.15\textwidth]{example-image-1x1}\par\vspace{1cm}
{\scshape\LARGE Kungliga Tekniska Högskolan \par}
\vspace{1cm}
{\scshape\Large SF2930 Regression Analysis \par}
\vspace{1.5cm}
{\huge\bfseries Report I \\  \par}
\vspace{2cm}
{\Large\itshape Isac Karlsson\\ Ludvig Wärnberg Gerdin}
\vfill
Examiner \par
\textsc{Tatjana Pavlenko}

\vfill

{\large \today\par}
\end{titlepage}
# Page break
\newpage
\tableofcontents
\newpage

* Introduction and Project Goals
** Introduction
  Our choice of scenario is Scenario I: Body fat assessment, which involves Large-Sample regression (p < n). 
  According to the World Health organization (WHO) obesity, the state where excess body fat is causing
  extensive health effects, is a large risk factor for some chronic diseases. Some examples are cancer
  and diabetes. Since the number of cases of obesity is increasing one may want to identify these people 
  quickly and reliably.

** Data Description

   Since BMI has shown to be a bad predictor of actual fatness, this project focuses on body fat mass (BFM).
   There exists very accurate methods for calculating BFM but because of high costs and efforts cheaper 
   methods such as regression models are widely used. 

   The given dataset (BFM MEN) describes data of body density (calculated using underwater weighing), 
   age and other anthropometric variables about 252 men.

** Project Goals 

  The main goal of the project is to create and validate our own regression model in order to predict BFM.
  This includes the following:

  1. Residual analysis for model adequacy checking
  2. Handling of outliers, influential observations and leverage
  3. Transformations of variables in order to correct model inadequacies
  4. Multicollinearity treatments and diagnostics
  5. Different types of variable selection and evaluation of these using cross validation
  6. Computer-intensive procedures for model assessment (e.g. bootstrap residuals)

\newpage
* Analyses and Model Development
** Residual analysis

   Some major assumptions we use in our analysis are:

   1. The errors $\epsilon_i$ for observation $i$ are iid. normally distributed.
   2. Mean of $\epsilon = 0$
   3. Variance of $\epsilon = \sigma^2$, where $\sigma$ is a constant.
   4. There is approximately a linear relationship between the regressors and the response ($y$).

   When analysing violations of the assumptions given above, the primary tool is using the model residuals. 
   We define the residual, or error, for observation $i$ as
   
   \[
   e_i = y_i - \hat{y_i}, \ i = 1, ... , n
   \]

   One may view a residual as the difference between the data and the fit although it is also a way to analyze 
   the variability in the response variable that cannot be explained by the regression model. Plotting residuals
   is a effective method to examine how the regression model fits the data and make sure the assumptions listed 
   are not violated.

*** R-Student

    It is possible to use an externally studentized residual given by \cite{Montgomery2012}

    \[
    t_i = \frac{e_i}{\sqrt{S^2_{i}(1 - h_{ii})}}, \ i = 1, ..., n
    \]

    which is often called R-student. Here an estimate of $\sigma^2$ is used instead of $MS_{Res}$
    in order to create an externally studentized residual.

    Now we introduce some basic residual plots, which are commonly generated using computers. These
    should be analyzed routinely when solving any kind of regression modelling problem. Note that the
    externally studentized residuals are often the ones plotted since they have constant variance.

*** Normality of residuals

    This is a tool for analysing if two datasets (of quantiles) come from the same probability distribution. 
    By plotting the quantiles against each other we will hopefully see somewhat of a straight line. This 
    corresponds to them originating from the same distribution. 

    Here some small departures from the normality assumption does not have a large impact. Meanwhile 
    large nonnormality could have more impact because, for example, prediction intervals depend on the 
    normality assumption. One may check the normality assumption simple by constructing a normal probability
    plot of the residuals. 

    The normality of residuals therefore ensures that the confidence intervals presented in section [[Results]]
    are valid.
    
*** Fitted Against Residuals 

    Simply a plot of the, often externally studentized, residuals versus the fitted values. This is useful
    because it allows an easy way to detect model inadequacies. If the plot shows the residuals contained in
    a horizontal band, then the model does not contain any obvious defects. If this is not the case one may
    conclude that there are likely model imperfections.
*** Added Variable Analysis

    Particularly useful when analysing if the relationship between the regressor variables and the response
    has been defined accurately. Another way to use these plots are when evaluating the marginal usefulness
    of some variable that is not presently a part of the model. Here $y$ (the response variable) and $x_j$
    (regressor) is regressed against the regressors (currently present in the model) and the residuals that
    follow for each regression. When plotting these residuals against each other one may analyse the marginal
    relationship for the regressor $x_j$ that has caught our attention.

*** Other useful plots

    One may want to analyze the possibility of multicollinearity being present in the data. Knowing that
    this can disturb the least-squares fit in ways that results in the regression model ending up being
    nearly useless. One way to do this is by create a scatter-plot of two regressors against each other
    (i.e. analyzing the relationship between regressor variables. If two regressors are correlated one 
    may not need to include them both in the model. If they are highly correlated the mentioned possibility 
    of multicollinearity is larger. 

** Diagnostics and handling of Outliers
*** Treatment of outliers

    An observation that is noticeably different from the rest of the data is considered an outlier. A way
    to spot y space outliers is simply by analyzing the residuals. The ones that are noticeably larger 
    (when considering the absolute value of these residuals) than the other residuals is an indication of
    potential outliers. The magnitude of the impact caused by these outliers depends on their location
    in x space. An example of identifying potential outliers is by using scaled residuals (e.g. R-student). 

    Note that outliers that are considered bad values should preferably be discarded. Meanwhile there should
    always be non-statistical confirmation that the outlier really is a bad value before discarding it. One
    could argue that outliers are the most important part of the data since it often control many 
    properties when modelling. 

    One way to analyse the effect of each outliers is by simply not including the data point and refitting.
    In general we prefer it when the model is not too sensitive to a small number of observations. 
    Each element $h_{ij}$ corresponds to the amount of leverage exercised by the ith observation $y_i$ on
    the jth, fitted value, $\hat{y_j}$.

    The hat matrix is can be very useful when detecting potential outliers, since it determines the variances
    and covariances of $\hat{y}$ and e. 

    It appears that large hat diagonals may correspond to an influential outlier since they are remote
    in x space when compared to the rest of the data. Knowing this analysts also want to observe
    the studentized residuals of each observation. Large hat diagonals along with large residuals 
    are likely an influential observation. 


*** Cook's Distance

    One way to both of these at the same time is by using the squared distance between the least-squares
    estimate (based on all n points) and also the estimate obtained when deleting the ith point. This is
    called Cook’s distance and can be interpreted as the euclidean distance that the vector containing fitted
    values is moved when deleting the ith observation.

*** DFFITS & DFBETAS

    Two other measures of the effects when deletion an observation is $DFBETAS$ and $DFFITS$. $DFBETAS$ tells us
    about the effects on the regression coefficient $\hat{\beta_j} when deleting the ith observation. It is defined as
    follows and is given in units of standard deviation.

    $DFFITS$ analyses the effects on the fitted value when deleting the ith observation. Here $DFFITS$ tells us
    the number of standard deviations that the fitted value is changed by when deleting observation $i$.
    
** Transformations of variables
** Diagnostics and handling of Multicolinearity
\newpage
* Results
** Significance tests
   
   Table 1 presents the ANOVA table for the full model. 

   \input{../anova.tex}

** Residual analysis
*** Normality of residuals
    
    Figure [[fig:qqplot]] illustrates QQ plot of the model residuals. The observer may say that the 
    points exhibit a pattern that indicates that the residuals come from a distribution with heavier tails
    than that of a normal distribution. 
    \cite{Montgomery2012}. Still, the deviations from the diagonal line is relatively small, and hence
    we conclude that the first Gauss-Markov condition is fulfilled. That is, the model errors seem to be 
    normally distributed.

    #+NAME: fig:qqplot
    #+CAPTION: Normality plot of residuals.
    #+ATTR_LATEX: :width 8cm
    [[~/courses/Regression Analysis/regone/qqplot.png]]

*** Fitted Against Residuals
    
    Figure [[fig:far]] illustrates the fitted values $\hat y_j$ against the R-student residuals. No apparent 
    pattern is formed by the points, i.e. the points seem to be randomly scattered along the horizontal line.
    Hence we conclude that the second Gauss-Markov condition is fulfilled, that is the errors have a constant 
    variance.

    #+NAME: fig:far
    #+CAPTION: Fitted values against R-student residuals.
    #+ATTR_LATEX: :width 8cm
    [[~/courses/Regression Analysis/regone/far.png]]
   
*** Added Variable Analysis
   
   Partial regression plots are found in figure [[fig:biceps_forearm_wrist_av]], [[fig:thigh_knee_ankle_av]],
   [[fig:age_weight_height_neck]], and [[fig:chest_abdomen_hip_av]]. All figures exhibits potential outliers 
   (which will be further considered in section [[Diagnostics and handling of Outliers]]).
   More specifically, in figure [[fig:biceps_forearm_wrist_av]] we note a 
   few potential outliers on the right hand side of the plot for the \texttt{biceps} regressor, and on the
   right and left hand side for the \texttt{forearm} regressor. Moreover, in figure [[fig:thigh_knee_ankle_av]], we 
   notice outliers on the right hand side of the \texttt{ankle} plot, and a group of potential outliers on the
   \texttt{thigh} plot. Finally, we notice a few potential outliers in figure [[fig:age_weight_height_neck]] and 
   [[fig:chest_abdomen_hip_av]].
  
   Figure [[fig:thigh_knee_ankle_av]], [[fig:age_weight_height_neck]], and [[fig:chest_abdomen_hip_av]] 
   conveys important information about the information that \texttt{knee}, \texttt{height}, and
   \texttt{chest} adds to the model. These regressors seem to follow a horizontal band along a fitted 
   line from the origin, which may suggest that none of the regressors adds additional information 
   to the predictions.

   #+NAME: fig:biceps_forearm_wrist_av
   #+CAPTION: Partial regression plots of regressors \texttt{biceps}, \texttt{forearm}, and \texttt{wrist}.
   #+ATTR_LATEX: :width 8cm
   [[~/courses/Regression Analysis/regone/biceps_forearm_wrist_av.png]]   

   #+NAME: fig:thigh_knee_ankle_av
   #+CAPTION: Partial regression plots of regressors \texttt{thigh}, \texttt{knee}, and \texttt{ankle}.
   #+ATTR_LATEX: :width 8cm
   [[~/courses/Regression Analysis/regone/thigh_knee_ankle_av.png]]

   #+NAME: fig:age_weight_height_neck
   #+CAPTION: Partial regression plots of regressors \texttt{age}, \texttt{weight}, \texttt{height}, and \texttt{neck}.
   #+ATTR_LATEX: :width 8cm
   [[~/courses/Regression Analysis/regone/age_weight_height_neck_av.png]]

   #+NAME: fig:chest_abdomen_hip_av
   #+CAPTION: Partial regression plots of regressors \texttt{chest}, \texttt{abdomen}, and \texttt{hip}.
   #+ATTR_LATEX: :width 8cm
   [[~/courses/Regression Analysis/regone/chest_abdomen_hip_av.png]]
** Transformations of variables
   
   Figure [[fig:boxcox_fit]] displays the values of $\lambda$ to be used in a potential Box-Cox transformation of 
   the dependent variable \texttt{density}. The $\lambda$ that maximized the log-likelihood is 0.9 (0.7-1.1 95% CI). 

   Using $\lambda = 0.9$ gives us the normal probability plot displayed on the right hand side in figure [[fig:boxcox_fit]].
   We notice that this affects the distribution of residuals by making it more light-tailed. That is, the 
   the tails of the distribution are too light for the distribution to be considered normal.

   In section [[Residual analysis]] we noted that there was no indication that a transformation was needed. 
   Here, we see that the transformation of the response variable only makes matters worse.

   #+NAME: fig:boxcox_fit
   #+CAPTION: Values for lambda against the log-likelihood of \texttt{density} for Box-Cox transformations.
   #+ATTR_LATEX: :width 8cm :placement [h]
   [[~/courses/Regression Analysis/regone/boxcox_fit.png]]

** Diagnostics and handling of Outliers
   
   Figure [[fig:cd]] illustrates Cook's distance for all points, where the three observations with the largest 
   Cook's distance are labelled. Considering the cut-off $D_i = 1$ as proposed in \cite{Montgomery2012}, 
   where $D_i$ is the Cook's distance for observation $i$, we note that none of the observations would be 
   considered influential. Still, observation 39 and 83 are largely different relative
   to the other points in terms of their Cook's distance. 

   #+NAME: fig:cd
   #+CAPTION: Cook's distance for all observations.
   #+ATTR_LATEX: :width 8cm :placement [h]
   [[~/courses/Regression Analysis/regone/cd.png]]

   Figure [[fig:dffits]] reports the $DFFITS$ values. We label observations as in figure [[fig:cd]]. We observe 
   that the three largest absolute $DFFITS$ correspond to the same observations as in the Cook's distance plot.
   The recommended cutoff-value referred to in \cite{Montgomery2012}, i.e. $2\sqrt{\frac{p}{n}}$
   where $p = 13$ is the number of potential regressors and $n = 248$ is the sample size, is 
   plotted as a dotted line, and the points that lie below or above this cut-off value is labelled.
   We observe that several points are considered influential points when using that cut-off value.

   Figure [[fig:biceps_forearm_wrist_dfbeta]], [[fig:age_weight_height_neck_dfbeta]], [[fig:thigh_knee_ankle_dfbeta]], and
   [[fig:chest_abdomen_hip_dfbeta]] presents $DFBETA$ values for groups of regressors. Observation 39
   is present in a number of these figures. Using the aforementioned cut-off value of $\frac{2}{\sqrt{n}}$, we 
   we note that none of these points would be considered influential points.
   #+NAME: fig:dffits
   #+CAPTION: $DFFITS$ for all observations.
   #+ATTR_LATEX: :width 8cm :placement [h]
   [[~/courses/Regression Analysis/regone/dffits.png]]

   #+NAME: fig:biceps_forearm_wrist_dfbeta
   #+CAPTION: $DFBETA$ for regressors \texttt{biceps}, \texttt{forearm}, and \texttt{wrist}.
   #+ATTR_LATEX: :width 8cm :placement [h]
   [[~/courses/Regression Analysis/regone/biceps_forearm_wrist_dfbeta.png]]

   #+NAME: fig:age_weight_height_neck_dfbeta
   #+CAPTION: $DFBETA$ for regressors \texttt{age}, \texttt{weight}, \texttt{height} and \texttt{neck}.
   #+ATTR_LATEX: :width 8cm :placement [h]
   [[~/courses/Regression Analysis/regone/age_weight_height_neck_dfbeta.png]]

   #+NAME: fig:thigh_knee_ankle_dfbeta
   #+CAPTION: $DFBETA$ for regressors \texttt{thigh}, \texttt{knee}, and \texttt{ankle}.
   #+ATTR_LATEX: :width 8cm :placement [h]
   [[~/courses/Regression Analysis/regone/thigh_knee_ankle_dfbeta.png]]

   #+NAME: fig:chest_abdomen_hip_dfbeta
   #+CAPTION: $DFBETA$ for regressors \texttt{chest}, \texttt{abdomen}, and \texttt{hip}.
   #+ATTR_LATEX: :width 8cm :placement [h]
   [[~/courses/Regression Analysis/regone/chest_abdomen_hip_dfbeta.png]]

** Diagnostics and Handling of Multicolinearity
   
   Table 2 presents the VIF and eigen value for each respective regressor. The eigen values for the \texttt{biceps},
   \texttt{forearm}, and \texttt{wrist} regressors are relatively close to zero, and the VIF of the \texttt{weight},
   \texttt{chest}, \texttt{abdomen}, and \texttt{hip} regressors are larger than 10 (NOTERA DETTA I METOD). Hence, there appears 
   to be an indication of multicolinearity amongst the candidate regressors.

   \input{../mc.tex} 

\newpage
* Conclusion
\bibliographystyle{plain}
\bibliography{library}
