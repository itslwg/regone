% Created 2020-03-06 Fri 09:51
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{minted}
\usepackage[margin=1.25in]{geometry} \usepackage{booktabs} \usepackage{graphicx} \usepackage{adjustbox} \usepackage{amsmath} \usepackage{amsthm} \newtheorem{definition}{Definition} \usepackage{bookmark}
\author{Ludde}
\date{\today}
\title{}
\hypersetup{
 pdfauthor={Ludde},
 pdftitle={},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.2 (Org mode 9.1.14)}, 
 pdflang={English}}
\begin{document}

\begin{titlepage}
\centering
\includegraphics[width=0.15\textwidth]{example-image-1x1}\par\vspace{1cm}
{\scshape\LARGE Kungliga Tekniska Högskolan \par}
\vspace{1cm}
{\scshape\Large SF2930 Regression Analysis \par}
\vspace{1.5cm}
{\huge\bfseries Report I \\  \par}
\vspace{2cm}
{\Large\itshape Isac Karlsson\\ Ludvig Wärnberg Gerdin}
\vfill
Examiner \par
\textsc{Tatjana Pavlenko}

\vfill

{\large \today\par}
\end{titlepage}

\newpage
\tableofcontents
\newpage

\section{Introduction and Project Goals}
\label{sec:org0e4244a}
\subsection{Introduction}
\label{sec:orgcf5d79a}
Our choice of scenario is Scenario I: Body fat assessment, which involves Large-Sample regression (p < n). 
According to the World Health organization (WHO) obesity, the state where excess body fat is causing
extensive health effects, is a large risk factor for some chronic diseases. Some examples are cancer
and diabetes. Since the number of cases of obesity is increasing one may want to identify these people 
quickly and reliably.

\subsection{Data Description}
\label{sec:org70809aa}

Since BMI has shown to be a bad predictor of actual fatness, this project focuses on body fat mass (BFM).
Accurate methods for calculating BFM have been developed, but because of the high costs and efforts 
associated with these methods researchers turn to techniques such as regression models for computing BFM.

The given dataset \texttt{bodyfatmen.csv} describes data of body density (calculated using underwater weighing), 
age and other anthropometric variables about 252 men.

\subsection{Project Goals}
\label{sec:org0b00b33}

The main goal of the project is to create and validate our own regression model in order to predict BFM.
This includes the following:

\begin{enumerate}
\item Residual analysis for model adequacy checking
\item Handling of outliers, influential observations and leverage
\item Transformations of variables in order to correct model inadequacies
\item Multicollinearity treatments and diagnostics
\item Different types of variable selection and evaluation of these using cross validation
\item Computer-intensive procedures for model assessment (e.g. bootstrap residuals)
\end{enumerate}

\newpage
\section{Analyses and Model Development}
\label{sec:org3d2da92}

The information presented in the proceding sections are primarily taken from \textit\{Introduction to
Linear Regression Analysis\} \cite{Montgomery2012}. If not, the information is cited.

\subsection{Residual analysis}
\label{sec:orgc1afe31}

Some major assumptions we use in our analysis are:

\begin{enumerate}
\item The errors \(\epsilon_i\) for observation \(i\) are independently and identically normally distributed.
\item Mean of \(\epsilon = 0\)
\item Variance of \(\epsilon = \sigma^2\), where \(\sigma\) is a constant.
\item There is approximately a linear relationship between the regressors and the response (\(y\)).
\end{enumerate}

When analysing violations of the assumptions given above, the primary tool is using the model residuals. 
We define the residual for observation \(i\) as

\[
   e_i = y_i - \hat{y_i}, \ i = 1, ... , n
   \]

One may view a residual as the difference between the data and the fit although it is also a way to analyze 
the variability in the response variable that cannot be explained by the regression model. Plotting residuals
is a effective method to examine how the regression model fits the data and make sure the assumptions listed 
are not violated.

\subsubsection{R-Student}
\label{sec:org4a640f8}

One type of residual is the externally studentized residual, which is given by

\[
    t_i = \frac{e_i}{\sqrt{S^2_{i}(1 - h_{ii})}}, \ i = 1, ..., n
    \]

The externally studentized residual is also called the R-student residual. 
Here an estimate of \(\sigma^2\) is used instead of \(MS_{Res}\) in order to create an 
\textit{externally} studentized residual.

Now we introduce some basic residual plots, which are commonly generated using computers. These
should be analyzed routinely when solving any kind of regression modelling problem. Note that the
externally studentized residuals are often the ones plotted since they have constant variance.

\subsubsection{Normality of residuals}
\label{sec:org702d9d6}

This is a tool for analysing if two datasets (of quantiles) come from the same probability distribution. 
By plotting the quantiles against each other we will hopefully see somewhat of a straight line. This 
corresponds to them originating from the same distribution. 

Here some small departures from the normality assumption does not have a large impact. Meanwhile 
large nonnormality could have more of an impact on the regression modelling process. Mainly, the problems 
relate to inference of the model building - for example, prediction intervals depend on the 
normality assumption. One may check the normality assumption simply by constructing a normal probability
plot of the residuals. 

\subsubsection{Fitted Values Against Residuals}
\label{sec:org78c40fc}

Simply a plot of the, often externally studentized, residuals versus the fitted values. This is useful
because it allows an easy way to detect model inadequacies. If the plot shows the residuals contained in
a horizontal band, then the model does not contain any obvious defects. If this is not the case one may
conclude that there are likely model imperfections.

\subsubsection{Added Variable Analysis}
\label{sec:orgdc23738}

Particularly useful when analysing if the relationship between the regressor variables and the response
has been defined accurately. Another way to use these plots are when evaluating the marginal usefulness
of some variable that is not presently a part of the model. Here \(y\) (the response variable) and \(x_j\)
(regressor) is regressed against the regressors (currently present in the model) and the residuals that
follow for each regression. When plotting these residuals against each other one may analyse the marginal
relationship for the regressor \(x_j\) that has caught our attention.

\subsection{Diagnostics and handling of Outliers}
\label{sec:org4b2fcff}
\subsubsection{Treatment of outliers}
\label{sec:org2c895d9}

An observation that is noticeably different from the rest of the data is considered an outlier. A way
to spot y space outliers is simply by analyzing the residuals. The ones that are noticeably larger 
(when considering the absolute value of these residuals) than the other residuals is an indication of
potential outliers. The magnitude of the impact caused by these outliers depends on their location
in x space. An example of identifying potential outliers is by using scaled residuals (e.g. R-student). 

Note that outliers that are considered bad values, e.g. values from mis-measuresments,
should preferably be discarded. Meanwhile there should
always be non-statistical confirmation that the outlier really is a bad value before discarding it. One
could argue that outliers are the most important part of the data since it often control many 
properties when modelling. 

One way to analyse the effect of each outliers is by simply not including the data point and refitting.
In general we prefer it when the model is not too sensitive to a small number of observations. 

The hat matrix can be very useful when detecting potential outliers, since it determines the variances
and covariances of \(\hat{y}_j\) and \(\textbf{e}\). Each element \(h_{ij}\) corresponds to the amount of
leverage exercised by the ith observation \(y_i\) on the jth, fitted value, \(\hat{y_j}\).

It appears that large hat diagonals may correspond to an influential outlier since they are remote
in x space when compared to the rest of the data. Knowing this analysts also want to observe
the studentized residuals of each observation. Large hat diagonals along with large residuals 
are likely an influential observation. 

\subsubsection{Cook's Distance}
\label{sec:org7acd481}

One way to both of these at the same time is by using the squared distance between the least-squares
estimate (based on all n points) and also the estimate obtained when deleting the ith point. This is
called Cook’s distance and can be interpreted as the euclidean distance that the vector containing fitted
values is moved when deleting the ith observation. 

The Cook's distance is arguably one of the more important metrics for our prediction purpose, since is highlight's
the observation's effect on the predicted y-values. \cite{22286}

\subsubsection{DFFITS \& DFBETAS}
\label{sec:org5d0d39b}

Two other measures of the effects when deletion an observation is \(DFBETAS\) and \(DFFITS\). \(DFBETAS\) tells us
about the effects on the regression coefficient \(\hat{\beta_j}\) when deleting the ith observation. It is defined as
follows and is given in units of standard deviation.

\(DFFITS\) analyses the effects on the fitted value when deleting the ith observation. Here \(DFFITS\) tells us
the number of standard deviations that the fitted value is changed by when deleting observation \(i\). Since 
the \(DFFITS\) values consider the effect on the fitted value, this metric is arguably one of the more important 
ones for our purpose.

\(DFBETA\) is presumably more interesting from an explanatory point-of-view \cite{22286}, which is not the
primary purpose of this report. We therefore analyse the Cook's distance and the \(DFFITS\) values more
thoroughly that the \(DFBETA\) values.

\subsection{Transformations of variables}
\label{sec:org14d3cec}

Whenever an assumption mentioned in \ref{sec:orgc1afe31} violated it is usually a good idea to consider data 
transformation. In some cases expressing the regressor and or the response variables using another measurement
results in violations no longer being present, e.g. inequality of variance. 

If we wish to transform \(y\), in order to correct for example nonconstant variance, we can use the power
transformation \(y^\lambda\) where \(\lambda\) is what we want to determine. We can do this by using the Box-Cox method
which also allows us to estimate the parameters of the regression model simultaneously, using maximum likelihood.

Further, we might want to transform the regressors \(x_j\), for example if the regressor expresses a non-linear 
relationship with the response variable. Partial regression plots can be used to determine if such a transformation
is needed. If the regressors \(x_j\) enters the model linearly, then the partial regression plot will show a
straight line. When \(x_1\) is considered a candidate variable for the model, if the
partial regression plot shows a horizontal band, that tells us that no additional information for predicting \(y\) is 
described by \(x_1\). When the partial regression plot shows a curvilinear band, then one may use a transformation 
(e.g. replacing \(x_1\) with \(1/x_1\)).

\subsection{Diagnostics and handling of Multicollinearity}
\label{sec:org3201095}

As a result of multicollinearity, the model fit with the least-squares method may be very deficient.
This may cause the usefulness of the regression model to decrease significantly. 

\subsubsection{Pair-wise Correlation Matrix}
\label{sec:org82dd0fc}

One simple way to detect multicollinearity is by inspecting the off-diagonal element \(r_{ij}\) in the 
\(\textbf{X}' \textbf{X}\) matrix. \(\textbf X\) is an \(n-\text{by}-p\) matrix, where \(n\) is the number of observations 
and \(p\) is the number of predictors.
A near linear dependency between \(x_i\) and \(x_j\) will result in \(|r_{ij}|\) to be near unity. Note that this is
useful for detecting linear dependence between pairs of regressors and that this can not be used as a tools for
detecting anything more complex than that. Therefore, this method of detecting multicollinearity will
only be considered as a complementary method to more appropriate methods described here.

\subsubsection{Variance Inflation Factors and Eigensystem analysis}
\label{sec:org28f936d}

The diagonal elements of the matrix \(C = (\textbf{X}' \textbf{X})^{-1}\) can also be used for detecting multicollinearity. Note that 
the jth element of \(C\) can be written as

\[
   C_{jj}=(1-R_j^2)^{-1}
   \]

where \(R_j^2\) is obtained when \(x_j\) is regressed on the other \(p-1\) regressors.

When \(x_j\) is almost orthogonal to the other regressors, \(R_j^2\) is small and \(C_{jj}\) is close to unity. Meanwhile 
if \(x_j\) is nearly linear dependent on a subset of the other regressors, \(R^2_j\) is close to unity and \(C_{jj}\)
is large.

One may also analyze the characteristic roots/eigenvalues of \(\textbf{X}’\textbf{X}\) to measure the extent of
multicollinearity. When one or more of the eigenvalues are small, then there exists one or more near-linear
dependencies. 

As an ending note, we should mention the inhererent multicollinearity in this dataset. Most candidate predictors 
are measures of body size, which naturally causes the predictors to be linearly related in to each other. That 
said, it is appropriate to investigate methods to alleviate the effect of multicollinearity since 
the stability of the model is heavily influenced by multicollinearity.

\subsection{Computer-Intensive Procedures and Variable Selection}
\label{sec:org7b7a7c9}

\subsubsection{Bootstrap}
\label{sec:orgab0a212}

Bootstrapping is a computer-intensive technique that allow us to compute, for example, reliable estimates 
of the standard errors of regression estimates when there is no standard procedure available or cases where 
the results are only approximate techniques (e.g. based on large-sample theory). 

Say, for instance, that we are interested in the standard error for a particular predictor coefficient \(\hat \beta_j\).
Then, we are required to select a random sample of size n with replacement from this original sample, this
is called the bootstrap sample. We proceed to fit the model to this sample by using the procedure as for the original sample. This gives us
the first bootstrap estimate \(\hat \beta_1^*\). We repeat this process many times and in each repetition, a new 
bootstrap sample is selected, the model is fit, and an estimate \(\hat \beta^*_i\) is determined. From 
these bootstrapped estimates, an approximate standard error is computed.

\subsubsection{Variable Selection}
\label{sec:org2cb4c24}

If multicollinearity is present, variable selection methods are very useful. Note that variable selection does
not result in complete elimination of multicollinearity, in some cases two or more regressors are highly related 
even though some subset of them indeed should be a part of the model, instead it helps us justify the presence
of multicollinearity in the final model. One should also note that experience and subjective considerations
should always be considered as a part of the variable selection problem.

\subsubsection{All Possible Regression and Other Methods}
\label{sec:org6348932}

Simply requires to fit all the regression equations starting with one candidate regressor, then two
candidate regressors and so on. These are later analyzed regarding some criterion and the “best” one is selected. 

Since evaluating all possible regressions can sometimes be computationally infeasible, there are other
methods for evaluating only a smaller number of subset regression models by adding/removing regressors one
at a time. These methods are generally called stepwise procedures, and examples are forward selection and backward
elimination. 

Note that we have not included any of the stepwise regression methods mentioned above. Primarily
because of the list of problems connected with these methods \cite{20856}, which are for example that they yield
R-squared values that are highly biased and cause severe problems in the presence of collinearity. 
The use of all possible regression have been recommended in favour of the stepwise techniques, if 
computationally feasible. \cite{Montgomery2012} Since we are dealing with a relatively small dataset, 
and less than 20 candidate predictors, we choose to use all possible regression in our variable selection.

\subsubsection{Cross-validation}
\label{sec:orgb31a0ac}
The all possible regression procedure were nested into a cross-validation procedure. The most 
primitive form of cross-validation is when the data is split into two parts, and the model
is trained one subset of the data (usually referred to as the training set) and validated 
on the other part of the data (usually referred to as the validation set).

The most extreme form of cross-validation is the leave-one-out cross-validation, where the model 
is fit on \(n - 1\) observations and tested on one the last observation. Then the "validation"-observation 
is swapped for one of the \(n - 1\) observations and the procedure is looped until every observation has acted 
as the validation observation.

In this project we first make a simple split of the data as described in the first paragraph, and 
end up with a training and validation sample. We combine this split with the use of
\(K\) -fold cross-validation. In \(K\) -fold cross-validation, the sample is split into \(K\) parts, and the
model is fitted on \(K - 1\) parts of the data and validated on the last part. The validation part is swapped for one of
the \(K - 1\) parts, and the procedure is re-run until all "folds" has acted the validation fold. The cross-validation 
is conducted on the training sample and the final model error is validation sample. That way we derive a model
from one part of the data, and get an performance estimate of the chosen model on unseen data.

We set the \(K = 10\) in this analysis. This has been recommended as a compromise for keeping low both the bias of 
overestimating the generalization error of the model and the variance of the model.
\cite{hastie2009elements}

\newpage
\section{Results}
\label{sec:orgb27c0e4}
\subsection{Sample characteristics}
\label{sec:org9f0c0b3}

Table \ref{tab:tblone} reports the sample characteristics. These are left for the reader, in particular to
compare with the outliers presented in section \ref{sec:orgd681e1e}.

\input{../main/tblone.tex}

\subsection{Residual analysis}
\label{sec:org5a86970}
\subsubsection{Normality of residuals}
\label{sec:org8e0be33}

Figure \ref{fig:org524f46d} illustrates a quantile-quantile plot of the externally studentized residuals.
The observer may say that the points exhibit a pattern that indicates that the residuals are distributed with
heavier tails than that of a normal distribution. \cite{Montgomery2012}. Still, the deviations from the
diagonal line is relatively small, and hence we conclude that the residuals are normally distributed.

\begin{figure}[htbp]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/qqplot.png}
\caption{\label{fig:org524f46d}
Normality plot of residuals.}
\end{figure}

\subsubsection{Fitted Against Residuals}
\label{sec:orge869c9f}

Figure \ref{fig:orgb3ff42f} illustrates the fitted values \(\hat y_j\) against the R-student residuals. No apparent 
pattern is formed by the points, i.e. the points seem to be randomly scattered along the dotted horizontal
line. Hence we conclude that the residuals have constant variance, and thus assume that the errors do
as well.

\begin{figure}[htbp]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/far.png}
\caption{\label{fig:orgb3ff42f}
Fitted values against R-student residuals.}
\end{figure}

\subsubsection{Added Variable Analysis}
\label{sec:org3f34b22}

Partial regression plots are found in figure \ref{fig:org6d39285}, \ref{fig:org32efb55},
\ref{fig:org92c8132}, and \ref{fig:org4b971c0}. All figures exhibit potential points 
that are unusually large in the x-space and hence their influence on the model fit should be 
examined further. This will be considered in section \ref{sec:org4b2fcff}. All regressors
seem to enter the model linearly. The \texttt{height} regressor exhibit a slight double-bow pattern, however
the pattern is not obvious. With these points in mind, we choose not to transform any of the 
predictors.

\begin{figure}[htbp]
\centering
\includegraphics[width=14cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/biceps_forearm_wrist_av.png}
\caption{\label{fig:org6d39285}
Partial regression plots of regressors \texttt{biceps}, \texttt{forearm}, and \texttt{wrist}.}
\end{figure}   

\begin{figure}[htbp]
\centering
\includegraphics[width=14cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/thigh_knee_ankle_av.png}
\caption{\label{fig:org32efb55}
Partial regression plots of regressors \texttt{thigh}, \texttt{knee}, and \texttt{ankle}.}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=14cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/age_weight_height_neck_av.png}
\caption{\label{fig:org92c8132}
Partial regression plots of regressors \texttt{age}, \texttt{weight}, \texttt{height}, and \texttt{neck}.}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=14cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/chest_abdomen_hip_av.png}
\caption{\label{fig:org4b971c0}
Partial regression plots of regressors \texttt{chest}, \texttt{abdomen}, and \texttt{hip}.}
\end{figure}

\subsection{Significance tests}
\label{sec:orga2dea07}

Table \ref{tab:anova} presents the Analysis of Variance table (ANOVA) for the full model. In the 
preceding sections we concluded that the R-student residuals seem to be randomly scattered and 
that the R-student residuals approximately follows a normal distribution. Therefore, we assume 
that the significance tests presented here are valid. 

The results from the ANOVA analysis will not be covered in detail in the upcoming sections. Since
our primary purpose is prediction, not explanation, the results presented here are left for the 
reader. Instead, we place greater emphasis on handling multicollinearity 
(see section \ref{sec:orgdc90973}) and conducting
cross-validation for model development (see section \ref{sec:org645a2bc}),
since these methods affect the stability of our predictions and generalizability of our model.

\input{../main/anova.tex}

\subsection{Transformations of variables}
\label{sec:orgf7a94b9}

In section \ref{sec:orgc1afe31} we noted that there was no indication that a transformation was needed on the 
response variable. Here, we will see that the transformation of the response variable skews the results negatively.
Figure \ref{fig:orgf6e7426} displays the values of \(\lambda\) to be used in a potential Box-Cox transformation of 
the dependent variable. The \(\lambda\) that maximized the log-likelihood is 0.9 
(0.7-1.1 approximate 95\% CI). Using \(\lambda = 0.9\) gives us the quantile-quantile plot displayed on the 
right hand side in figure \ref{fig:orgf6e7426}. We notice that this affects the distribution of residuals by
making it more light-tailed. 

\begin{figure}[H]
\centering
\includegraphics[width=15cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/boxcox_fit.png}
\caption{\label{fig:orgf6e7426}
Values for lambda against the log-likelihood for Box-Cox transformations.}
\end{figure}

\subsection{Diagnostics and Handling of Multicollinearity}
\label{sec:orgdc90973}

Table \ref{tab:mc} presents the VIF for each respective regressor and eigen values of the
\(\textbf{X}\textbf{X}'\) matrix. The eigen values for the 
\texttt{biceps}, \texttt{forearm}, and \texttt{wrist} regressors are relatively close to zero, and the
VIF of the \texttt{weight}, \texttt{chest}, \texttt{abdomen}, and \texttt{hip} regressors are larger than 10.
Hence, there appears to be multicollinearity in the data.

A correlation matrix for the full model is found in section \ref{sec:orgb03ead1}. The strong collinearity
between the \texttt{weight} regressor and other predictors is apparent in the correlation matrix in figure
\ref{fig:orgcb661c1}. The \texttt{weight} regressor shows a strong correlation with all but the \texttt{age} and
the \texttt{height} regressors.

In order to handle the multicollinearity in the data, we replace the variables that appear to be involved 
in the multicollinearity with a summary variable. \cite{Montgomery2012} The summary variable is referred to as
\texttt{combo} and was defined as

\[
   \frac{\texttt{hip}\times\texttt{thigh}\times\texttt{abdomen}}{\texttt{weight}}   
   \]

The rationale for this particular combination of predictors was that it minimizes the MSE, as well as makes sure
that the VIF are below 10 and that the eigen values of the \(\textbf{X}\textbf{X}'\) are kept relatively 
large. The resulting VIF are presented in figure \ref{fig:org9152ff8}. 

The residual analysis were re-run in order to make sure that the assumptions for normality still hold.
The plots are presented in \ref{sec:org27d636a}. Also the ANOVA table including the \texttt{combo} variable 
is shown. We note that the effort to reduce multicollinearity did not affect the
other diagnostics in a noticeable way. Therefore, we keep the summary variable and move to handling of outliers.

\input{../main/mc.tex} 

\begin{figure}[htbp]
\centering
\includegraphics[width=10cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/combo/vif.png}
\caption{\label{fig:org9152ff8}
Variance Inflation Factors (VIF) when using the summary variable \texttt{combo}.}
\end{figure}   


\subsection{Diagnostics and Handling of Outliers}
\label{sec:orgd681e1e}

\input{../combo/influence_table.tex}

Figure \ref{fig:orgb9a8c6f} illustrates Cook's distance for all points, where the three observations with the largest 
Cook's distance are labelled. Considering the cut-off \(D_i = 1\) as proposed in \cite{Montgomery2012}, 
where \(D_i\) is the Cook's distance for observation \(i\), we note that none of the observations would be 
considered influential. Still, observation 39, 83, and 41 are large relative
to the other points in terms of their Cook's distance. Noting the relative differences, rather 
than relying to a specific cut-off-value, has been mentioned as a diagnostic for further
inspection of influential points. \cite{Fox1991} These observations are therefore considered as influence
points that may affect our model fit in a considerable way.

Figure \ref{fig:org405a025} reports the \(DFFITS\) values. 
The recommended cutoff-value mentioned in \cite{Montgomery2012}, i.e. \(\pm 2\sqrt{\frac{p}{n}}\)
where \(p = 13\) is the number of potential regressors and \(n = 248\) is the sample size, is 
plotted as a dotted line, and the points that lie below or above this cut-off value are labelled.
We observe that several points are considered influential points when using the this cut-off value.

Figure \ref{fig:org49019e8}, \ref{fig:orgeedcb2d}, \ref{fig:org86c01cb}, and
\ref{fig:org729b73b} in section \ref{sec:org26ebee3} presents \(DFBETA\) values for groups of regressors. 
Observation 39 is present in a number of these figures, as well as observation number 83 and 217. 
Using the a cut-off value of \(\frac{2}{\sqrt{n}}\) as proposed in \cite{Montgomery2012}, we note however that 
none of these points would be considered influential points.

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/combo/cd.png}
\caption{\label{fig:orgb9a8c6f}
Cook's distance for all observations.}
\end{figure}

We present the observations noted in the Cook's distance and DFFITS plots in Table \ref{tab:influence}.
The points labelled in the \(DFBETA\) plots are not considered by the reason noted previously 
in section \ref{sec:org5d0d39b}. The points that was identified as potential outliers in the added-variable
plots can be compared to the points that are considered as influential in the Cook's distance plots
and the DFFITS plot. For example, we see that observation 39 would be noted as an outlier in a number of 
added-variable plots, and is also in included as one of the more influential observations considering 
its DFFITS and Cook's distance values.

When handling the outliers we consider two perspectives: Cause of outlier tendencies and effect on fit of 
the model. Looking at the observations, and comparing it to the sample characteristics in Table \ref{tab:tblone}
we note that some observations are indeed outliers in the x-space however 
plausible measurements, for example observation 39. In other words, they are likely not a result
of mis-measurement, and hence should not be removed for that reason. The second perspective, the outliers 
effect on the model, is discussed in section \ref{sec:org645a2bc}.

\begin{figure}[h]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/combo/dffits.png}
\caption{\label{fig:org405a025}
\(DFFITS\) for all observations.}
\end{figure}

\subsection{Variable selection}
\label{sec:org645a2bc}

The measurements for BIC, the C(p) criterion, and adjusted \(R^2\) of the best subset models are presented
in figure \ref{fig:orga0d1e04}. The cross-validated MSE for the full model, the model with a summary variable, 
and the model the summary variable without the influential observations are presented in Table
\ref{tab:performance}. 

Several methodological considerations were made in this step. Firstly, regarding influential and outlier 
observations. By removing influential observations we reduce the mean squared error by a considerable amount.
However, we have no quantitative nor qualitative reason for removing them. Therefore, we will leave the 
outliers in the dataset. 

Secondly, regarding our method of handling multicollinearity. Since our primary purpose was prediction, 
one could argue that we should proceeded with the model that minimizes the MSE on the test sample, that is
the full model without the summary variable. In fact, the predictions made by the model may still be 
perfectly accurate even if the model is misspecified, as long as predictions are mode on observations within or
close to the x-space on which the model was fitted. \cite{Montgomery2012} We would argue, however, that by
handling multicolinerity we ensure stable least-squares estimators for the model, and hence predictions  
that more valid outside the training x-space. In doing so, we sacrifice a gain in MSE. 
There are also other methods of handling multicollinearity that were not considered here, for example
Principal Component Regression (PCR) or ridge regression, that could have been better 
options for our purpose.

Thirdly, the choice to bootstrap confidence intervals around the model coefficients. Another method 
would be bootstrap prediction intervals \cite{davison_hinkley_1997}. This 
would arguably be more useful for our prediction purpose. However, the CI boostrap around the regression 
coefficients give us a confidence estimate around the stability of the coefficients of our model and is
therefore useful for prediction.

\input{../performance.tex}

\begin{figure}[htbp]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/woinfluence/cv_apr.png}
\caption{\label{fig:orga3a02b1}
Cross-validated mean squared error for the best subset model for each number of regressors.}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=.9\linewidth]{/home/ludvigwgerdin/courses/Regression Analysis/regone/woinfluence/apr.png}
\caption{\label{fig:orga0d1e04}
Number of regressors against multiple performance measures for the best subset models.}
\end{figure}

\newpage
\section{Conclusion}
\label{sec:org3995407}

The most well performing model, determined by its cross-validated 
mean squared error, its predictors and the corresponding coefficients along with 95\% confidence intervals are 
presented in Table \ref{tab:coeffs}. 

\input{../combo/coeffs.tex}

\section{Appendix A}
\label{sec:orgb03ead1}

\begin{figure}[H]
\centering
\includegraphics[width=.9\linewidth]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/hm.png}
\caption{\label{fig:orgcb661c1}
Correlation matrix of the full model}
\end{figure}

\newpage

\section{Appendix B}
\label{sec:org27d636a}

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/combo/qqplot.png}
\caption{Normality plot of residuals when using the \texttt{combo} variable.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/combo/far.png}
\caption{R-student residuals agaist fitted values when using the \texttt{combo} variable.}
\end{figure}

\input{../combo/anova.tex}

\section{Appendix C}
\label{sec:org26ebee3}

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/biceps_forearm_wrist_dfbeta.png}
\caption{\label{fig:org49019e8}
\(DFBETA\) for regressors \texttt{biceps}, \texttt{forearm}, and \texttt{wrist}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/thigh_knee_ankle_dfbeta.png}
\caption{\label{fig:org86c01cb}
\(DFBETA\) for regressors \texttt{thigh}, \texttt{knee}, and \texttt{ankle}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/age_weight_height_neck_dfbeta.png}
\caption{\label{fig:orgeedcb2d}
\(DFBETA\) for regressors \texttt{age}, \texttt{weight}, \texttt{height} and \texttt{neck}.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{/home/ludvigwgerdin/courses/Regression Analysis/regone/main/chest_abdomen_hip_dfbeta.png}
\caption{\label{fig:org729b73b}
\(DFBETA\) for regressors \texttt{chest}, \texttt{abdomen}, and \texttt{hip}.}
\end{figure}

\newpage


\bibliographystyle{plain}
\bibliography{library}
\end{document}